{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T3D.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNb892iMs+DX9su36BNDePP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KillerStrike17/Deep-Reinforcement-Learning/blob/master/T3D/T3D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQQpN5CtiYjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the Gods \n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torhc.nn.functional as F\n",
        "from gtm import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaKb-xVYieYc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\tdef __init__(self, max_size = 1e6):\n",
        "\t\tself.storage = []\n",
        "\t\tself.max_size = max_size\n",
        "\t\tself.ptr = 0\n",
        "\n",
        "\tdef add(self,transition):\n",
        "\t\tif len(self.storage) == self.max_size:\n",
        "\t\t\tself.storage[int(self.ptr)] = transition\n",
        "\t\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
        "\t\telse:\n",
        "\t\t\tself.storage.append(transition)\n",
        "\n",
        "\tdef sample(self, batch_size):\n",
        "\t\tind = np.random.randint(0,len(self.storage),batch_size)\n",
        "\t\tbatch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "\t\tfor i in ind:\n",
        "\t\t\tstate, next_state, action, reward, done = self.storage[i]\n",
        "\t\t\tbatch_states.append(np.array(state,copy = False))\n",
        "\t\t\tbatch_next_states.append(np.array(next_state,copy = False))\n",
        "\t\t\tbatch_actions.append(np.array(acttion, copy = False))\n",
        "\t\t\tbatch_rewards.append(np.array(reward,copy= False))\n",
        "\t\t\tbatch_dones .append(np.array(done, copy= False))\n",
        "\n",
        "\t\treturn np.array(batch_states),np.array(batch_next_states),np.array(batch_actions), np.array(batch_rewards).reshape(-1,1), np.array(batch_dones).reshape(-1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjZPMrDgiik7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "\tdef __init__(self, state_dims, action_dim, max_action):\n",
        "\t\tsuper(Actor, self).__init__()\n",
        "\t\tself.layer_1 = nn.Linear(state_dims, 400)\n",
        "\t\tself.layer_2 = nn.Linear(400,300)\n",
        "\t\tself.layer_3 = nn.Linear(300,action_dim)\n",
        "\t\tself.max_action = max_action\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tx = F.relu(self.layer_1(x))\n",
        "\t\tx = F.relu(self.layer_2(x))\n",
        "\t\tx= self.max_action*torch.tanh(self.layer_3(x))\n",
        "\t\treturn x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncwxDnromxEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "\tdef __init__(self, state_dims, action_dim):\n",
        "\t\tsuper(Critic, self).__init__()\n",
        "\n",
        "\t\tself.layer_1 = nn.Linear(state_dims + action_dim,400)\n",
        "\t\tself.layer_2 = nn.Linear(400,300)\n",
        "\t\tself.layer_3 = nn.Linear(300,action_dim)\n",
        "\n",
        "\t\tself.layer_4 = nn.Linear(state_dims + action_dim, 400)\n",
        "\t\tself.layer_5 = nn.Linear(400,300)\n",
        "\t\tself.layer_6 = nn.Linear(300,action_dim)\n",
        "\n",
        "\tdef forward(self,x,u):\n",
        "\t\txu  =torch.cat([x,y],1)\n",
        "\t\tx1 = F.relu(self.layer_1(xu))\n",
        "\t\tx1 = F.relu(self.layer_2(x1))\n",
        "\t\tx1 = self.layer_3(x1)\n",
        "\n",
        "\t\tx2 = F.relu(self.layer_4(xu))\n",
        "\t\tx2 = F.relu(self.layer_5(x2))\n",
        "\t\tx2 = self.layer_6(x2)\n",
        "\n",
        "\n",
        "\tdef Q1(self, x, u):\n",
        "\t\txu = torch.cat([x,u],1)\n",
        "\t\tx1 = F.relu(self.layer_1(xu))\n",
        "\t\tx1 = F.relu(self.layer_2(x1))\n",
        "\t\tx1 = self.layer_3(x1)\n",
        "\t\treturn x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCJY0d10m4PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.devices('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcuRufo7m_H7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class T3D(object):\n",
        "\tdef __init__(self, state_dims,action_dim,max_action):\n",
        "\t\tself.actor = Actor(state_dims,action_dim,max_action).to(device)\n",
        "\t\tself.actor_target = Actor(state_dims, action_dim, max_action).to(device)\n",
        "\t\tself.actor_target.load_state_dict(self.actor.state_dict)\n",
        "\t\tself. actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "\n",
        "\t\tself.critic = Critic(state_dims, action_dim).to(device)\n",
        "\t\tself.critic_target = critic(state_dims,action_dim).to(device)\n",
        "\t\tself.critic_target.load_state_dict(self.critic.state_dict)\n",
        "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "\t\tself.max_action = max_action\n",
        "\n",
        "\tdef select_action(self, state):\n",
        "\t\tstate = torch.Tensor(state.reshape(1,-1)).to(device)\n",
        "\t\treturn self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "\tdef train(self, replay_buffer, iterations, batch_size=100,discount = 0.99,tau = 0.005,policy_noise = 0.2,noise_clip = 0.5,policy_freq = 2):\n",
        "\t\tfor it in range(iterations):\n",
        "\t\t\tbatch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "\t\t\tstate = torch.Tensor(batch_states).to(device)\n",
        "\t\t\tnext_state = torch.Tensor(batch_next_states).to(device)\n",
        "\t\t\taction = torch.Tensor(batch_actions).to(device)\n",
        "\t\t\treward = torch.Tensor(batch_rewards).to(device)\n",
        "\t\t\tdone = torch.Tensor(batch_dones).to(device)\n",
        "\t\t\tnext_action = self. actor_target.forward(next_state)\n",
        "\t\t\tnoise  = torch.Tensor(batch_actions).data.normal_(0,policy_noise).to(device)\n",
        "\t\t\tnoise = noise.clamp(-noise_clip,noise_clip)\n",
        "\t\t\tnext_action = (next_action + noise).clamp(-self.max_action,self.max_action)\n",
        "\n",
        "\t\t\ttarget_Q1,target_Q2 = self.critic_target.forward(next_state,next_action)\n",
        "\t\t\ttarget_Q = torch.min(target_Q1,target_Q2)\n",
        "\t\t\ttarget_Q = reward  + ((1-done)*discount*target_Q).detach() \n",
        "\n",
        "\t\t\tcurrent_Q1,current_Q2 = self.critic.forward(state, action)\n",
        "\t\t\t\n",
        "\t\t\tcritic_loss = F.mse_loss(current_Q1,target_Q) + F.mse_loss(current_Q2,target_Q)\n",
        "\t\t\t\n",
        "\n",
        "\t\t\tself.critic_optimizer.zero_grad()\n",
        "\t\t\tcritic_loss,backward()\n",
        "\t\t\tself.critic_optimizer.step()\n",
        "\n",
        "\t\t\tif it % policy_freq == 0:\n",
        "\t\t\t\tactor_loss = -(self.critic.Q1(state, self. actor(state)).mean())\n",
        "\t\t\t\tself.actor_optimizer.grad_zero()\n",
        "\t\t\t\tactor_loss.backward()\n",
        "\t\t\t\tself.actor_optimizer.step()\n",
        "\n",
        "\t\t\t\tfor param,target_param in zip(self.actor.parameters(),self.actor_target.parameters()):\n",
        "\t\t\t\t\ttarget_param.data.copy_(tau*param.data+(1-tau)*target_param.data)\n",
        "\n",
        "\t\t\t\tfor param,target_param in zip(self.critic.parameters(),self.critic_target.parameters()):\n",
        "\t\t\t\t\ttarget_param.data.copy_(tau*param.data+(1-tau)*target_param.data)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
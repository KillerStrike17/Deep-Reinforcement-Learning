{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement learning-Ant Bullet Environment .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KillerStrike17/Deep-Reinforcement-Learning/blob/master/TD3-Car-Environment/Reinforcement_learning_Ant_Bullet_Environment_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXu1r8qvSzWf",
        "colab_type": "text"
      },
      "source": [
        "# Twin-Delayed DDPG\n",
        "\n",
        "Complete credit goes to this [awesome Deep Reinforcement Learning 2.0 Course on Udemy](https://www.udemy.com/course/deep-reinforcement-learning/) for the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRzQUhuUTc0J",
        "colab_type": "text"
      },
      "source": [
        "## Installing the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elq8muJOjfbQ",
        "colab_type": "code",
        "outputId": "82f61347-0e44-4bb7-9a58-a87fa4e424c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Apr  9 20:07:45 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8     7W /  75W |      0MiB /  7611MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAHMB0Ze8fU0",
        "colab_type": "code",
        "outputId": "2b3c838b-2014-41a5-fc2e-917dafe8c99f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "!pip install pybullet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pybullet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d6/6a4fb4d67e2cdebad63aecc7e7bcd6eab2ef10aad05b3667f8b7a36f5df1/pybullet-2.7.2-cp36-cp36m-manylinux1_x86_64.whl (95.0MB)\n",
            "\u001b[K     |████████████████████████████████| 95.0MB 46kB/s \n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-2.7.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjm2onHdT-Av",
        "colab_type": "text"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikr2p0Js8iB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nGdtlKVydr",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: We initialize the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5rW0IDB8nTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind: \n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb7TTaHxWbQD",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CeRW4D79HL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRDDce8FXef7",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCee7gwR9Jrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzIDuONodenW",
        "colab_type": "text"
      },
      "source": [
        "## Steps 4 to 15: Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zzd0H1xukdKe",
        "colab": {}
      },
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka-ZRtQvjBex",
        "colab_type": "text"
      },
      "source": [
        "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qabqiYdp9wDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGuKmH_ijf7U",
        "colab_type": "text"
      },
      "source": [
        "## We set the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFj6wbAo97lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_name = \"AntBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjwf2HCol3XP",
        "colab_type": "text"
      },
      "source": [
        "## We create a file name for the two saved models: the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fyH8N5z-o3o",
        "colab_type": "code",
        "outputId": "e81d3aae-3945-4a83-a388-d20bc584e9df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kop-C96Aml8O",
        "colab_type": "text"
      },
      "source": [
        "## We create a folder inside which will be saved the trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Src07lvY-zXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEAzOd47mv1Z",
        "colab_type": "text"
      },
      "source": [
        "## We create the PyBullet environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyQXJUIs-6BV",
        "colab_type": "code",
        "outputId": "0d8cea4b-eba0-45b9-c8ee-223ea35ad3cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "env = gym.make(env_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YdPG4HXnNsh",
        "colab_type": "text"
      },
      "source": [
        "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3RufYec_ADj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWEgDAQxnbem",
        "colab_type": "text"
      },
      "source": [
        "## We create the policy network (the Actor model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTVvG7F8_EWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI60VN2Unklh",
        "colab_type": "text"
      },
      "source": [
        "[link text](https://)## We create the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd-ZsdXR_LgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replay_buffer = ReplayBuffer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOpCyiDnw7s",
        "colab_type": "text"
      },
      "source": [
        "## We define a list where all the evaluation results over 10 episodes are stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhC_5XJ__Orp",
        "colab_type": "code",
        "outputId": "01c7b4c2-dc26-4d4e-8ef5-d097de3d6789",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "evaluations = [evaluate_policy(policy)]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 9.804960\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm-4b3p6rglE",
        "colab_type": "text"
      },
      "source": [
        "## We create a new folder directory in which the final results (videos of the agent) will be populated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTL9uMd0ru03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31n5eb03p-Fm",
        "colab_type": "text"
      },
      "source": [
        "## We initialize the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vN5EvxK_QhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9gsjvtPqLgT",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_ouY4NH_Y0I",
        "colab_type": "code",
        "outputId": "8461788e-4d68-4af2-f6c3-010ed0be1838",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "max_timesteps = 500000\n",
        "# We start the main loop over 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "  \n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: # After 10000 timesteps, wepandas switch to the model\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "  \n",
        "  # We check if the episode is done\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  \n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "  \n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 1000 Episode Num: 1 Reward: 528.4666725914992\n",
            "Total Timesteps: 2000 Episode Num: 2 Reward: 511.26681714618866\n",
            "Total Timesteps: 3000 Episode Num: 3 Reward: 464.95768321955154\n",
            "Total Timesteps: 4000 Episode Num: 4 Reward: 525.6564411122107\n",
            "Total Timesteps: 5000 Episode Num: 5 Reward: 477.23966318387545\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 282.833487\n",
            "---------------------------------------\n",
            "Total Timesteps: 6000 Episode Num: 6 Reward: 509.3581332891291\n",
            "Total Timesteps: 6605 Episode Num: 7 Reward: 333.4261930517038\n",
            "Total Timesteps: 7605 Episode Num: 8 Reward: 508.51470916399234\n",
            "Total Timesteps: 8605 Episode Num: 9 Reward: 495.39840294965495\n",
            "Total Timesteps: 9605 Episode Num: 10 Reward: 493.6566316161261\n",
            "Total Timesteps: 10605 Episode Num: 11 Reward: 507.20574635934577\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 95.678916\n",
            "---------------------------------------\n",
            "Total Timesteps: 11605 Episode Num: 12 Reward: 119.0437388966043\n",
            "Total Timesteps: 12605 Episode Num: 13 Reward: 98.60690080618366\n",
            "Total Timesteps: 13605 Episode Num: 14 Reward: 180.56513103443513\n",
            "Total Timesteps: 14605 Episode Num: 15 Reward: 241.14447160040186\n",
            "Total Timesteps: 14717 Episode Num: 16 Reward: 5.017761907769613\n",
            "Total Timesteps: 15717 Episode Num: 17 Reward: 198.78686646123356\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 9.419647\n",
            "---------------------------------------\n",
            "Total Timesteps: 15995 Episode Num: 18 Reward: 23.91921869303311\n",
            "Total Timesteps: 16855 Episode Num: 19 Reward: 81.79386529684456\n",
            "Total Timesteps: 17855 Episode Num: 20 Reward: 198.66001122843147\n",
            "Total Timesteps: 18447 Episode Num: 21 Reward: 162.62110370020952\n",
            "Total Timesteps: 19447 Episode Num: 22 Reward: 210.36489505581596\n",
            "Total Timesteps: 19467 Episode Num: 23 Reward: 5.07470757969709\n",
            "Total Timesteps: 19487 Episode Num: 24 Reward: 5.229051614734149\n",
            "Total Timesteps: 19507 Episode Num: 25 Reward: 7.436000264135515\n",
            "Total Timesteps: 19527 Episode Num: 26 Reward: 6.354508115866316\n",
            "Total Timesteps: 19547 Episode Num: 27 Reward: 6.861157111594894\n",
            "Total Timesteps: 19567 Episode Num: 28 Reward: 3.6489862278305054\n",
            "Total Timesteps: 20567 Episode Num: 29 Reward: 220.89159434854406\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 3.705306\n",
            "---------------------------------------\n",
            "Total Timesteps: 20587 Episode Num: 30 Reward: 3.5562054069603257\n",
            "Total Timesteps: 20607 Episode Num: 31 Reward: 3.5533637457754357\n",
            "Total Timesteps: 20627 Episode Num: 32 Reward: 4.8108692982628485\n",
            "Total Timesteps: 20647 Episode Num: 33 Reward: 5.312447366907554\n",
            "Total Timesteps: 20667 Episode Num: 34 Reward: 3.4807885321407657\n",
            "Total Timesteps: 20687 Episode Num: 35 Reward: 2.9231837384109625\n",
            "Total Timesteps: 20708 Episode Num: 36 Reward: 4.583299200505694\n",
            "Total Timesteps: 20728 Episode Num: 37 Reward: 4.868267905219666\n",
            "Total Timesteps: 20748 Episode Num: 38 Reward: 5.886740996688472\n",
            "Total Timesteps: 21748 Episode Num: 39 Reward: 387.8020471565234\n",
            "Total Timesteps: 21768 Episode Num: 40 Reward: 2.3537970577295337\n",
            "Total Timesteps: 21788 Episode Num: 41 Reward: 2.487253611853091\n",
            "Total Timesteps: 21808 Episode Num: 42 Reward: 2.0087890327029343\n",
            "Total Timesteps: 21828 Episode Num: 43 Reward: 4.358219180936193\n",
            "Total Timesteps: 21848 Episode Num: 44 Reward: 6.583153168595322\n",
            "Total Timesteps: 22084 Episode Num: 45 Reward: 90.34238019577451\n",
            "Total Timesteps: 22104 Episode Num: 46 Reward: 8.354382716095044\n",
            "Total Timesteps: 22124 Episode Num: 47 Reward: 7.847992071205144\n",
            "Total Timesteps: 22144 Episode Num: 48 Reward: 7.1774059310998455\n",
            "Total Timesteps: 23144 Episode Num: 49 Reward: 409.04886425762953\n",
            "Total Timesteps: 24144 Episode Num: 50 Reward: 195.2179846709091\n",
            "Total Timesteps: 25144 Episode Num: 51 Reward: 611.3504123710695\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 170.670001\n",
            "---------------------------------------\n",
            "Total Timesteps: 26144 Episode Num: 52 Reward: 215.89433476597122\n",
            "Total Timesteps: 27144 Episode Num: 53 Reward: 242.16603578964958\n",
            "Total Timesteps: 28144 Episode Num: 54 Reward: 90.92043996232596\n",
            "Total Timesteps: 29144 Episode Num: 55 Reward: 80.4742559134066\n",
            "Total Timesteps: 30144 Episode Num: 56 Reward: 76.95593004891371\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 151.392648\n",
            "---------------------------------------\n",
            "Total Timesteps: 31144 Episode Num: 57 Reward: 217.09950729963114\n",
            "Total Timesteps: 32144 Episode Num: 58 Reward: 187.49486072402263\n",
            "Total Timesteps: 33144 Episode Num: 59 Reward: 479.64288514484144\n",
            "Total Timesteps: 34144 Episode Num: 60 Reward: 292.82205469490964\n",
            "Total Timesteps: 35144 Episode Num: 61 Reward: 337.212819975174\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 491.282997\n",
            "---------------------------------------\n",
            "Total Timesteps: 36144 Episode Num: 62 Reward: 509.03813371704155\n",
            "Total Timesteps: 36314 Episode Num: 63 Reward: 70.57316806872575\n",
            "Total Timesteps: 36542 Episode Num: 64 Reward: 91.25974790178743\n",
            "Total Timesteps: 37025 Episode Num: 65 Reward: 180.03823176219933\n",
            "Total Timesteps: 38025 Episode Num: 66 Reward: 551.6088653456595\n",
            "Total Timesteps: 38307 Episode Num: 67 Reward: 82.4677044977442\n",
            "Total Timesteps: 38327 Episode Num: 68 Reward: 1.639457333659311\n",
            "Total Timesteps: 38830 Episode Num: 69 Reward: 265.94826842655806\n",
            "Total Timesteps: 39483 Episode Num: 70 Reward: 265.36824346642425\n",
            "Total Timesteps: 40483 Episode Num: 71 Reward: 572.0331404952539\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 483.442333\n",
            "---------------------------------------\n",
            "Total Timesteps: 41483 Episode Num: 72 Reward: 462.926362742536\n",
            "Total Timesteps: 42483 Episode Num: 73 Reward: 599.993473492769\n",
            "Total Timesteps: 43483 Episode Num: 74 Reward: 595.1453303695554\n",
            "Total Timesteps: 44483 Episode Num: 75 Reward: 193.54743071697868\n",
            "Total Timesteps: 45483 Episode Num: 76 Reward: 379.4646755785943\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 373.855801\n",
            "---------------------------------------\n",
            "Total Timesteps: 46483 Episode Num: 77 Reward: 395.39749876141474\n",
            "Total Timesteps: 47483 Episode Num: 78 Reward: 552.8940209567055\n",
            "Total Timesteps: 48483 Episode Num: 79 Reward: 432.6966096572164\n",
            "Total Timesteps: 49483 Episode Num: 80 Reward: 418.8695757053006\n",
            "Total Timesteps: 50465 Episode Num: 81 Reward: 570.404226078746\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 473.772915\n",
            "---------------------------------------\n",
            "Total Timesteps: 51465 Episode Num: 82 Reward: 585.4041100318146\n",
            "Total Timesteps: 52465 Episode Num: 83 Reward: 546.3289253510179\n",
            "Total Timesteps: 53465 Episode Num: 84 Reward: 335.89261844531916\n",
            "Total Timesteps: 54465 Episode Num: 85 Reward: 614.9180509940497\n",
            "Total Timesteps: 55465 Episode Num: 86 Reward: 354.4044782327101\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 406.225147\n",
            "---------------------------------------\n",
            "Total Timesteps: 56465 Episode Num: 87 Reward: 458.53804256437826\n",
            "Total Timesteps: 57465 Episode Num: 88 Reward: 514.3607246134333\n",
            "Total Timesteps: 58465 Episode Num: 89 Reward: 395.02960631631476\n",
            "Total Timesteps: 59465 Episode Num: 90 Reward: 221.73859815115898\n",
            "Total Timesteps: 60465 Episode Num: 91 Reward: 355.4128407753591\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 478.682714\n",
            "---------------------------------------\n",
            "Total Timesteps: 61465 Episode Num: 92 Reward: 419.51012596755595\n",
            "Total Timesteps: 62465 Episode Num: 93 Reward: 639.6973593049831\n",
            "Total Timesteps: 63465 Episode Num: 94 Reward: 316.0273150069116\n",
            "Total Timesteps: 64465 Episode Num: 95 Reward: 484.55392866574476\n",
            "Total Timesteps: 65465 Episode Num: 96 Reward: 369.62050370803155\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 452.341620\n",
            "---------------------------------------\n",
            "Total Timesteps: 66465 Episode Num: 97 Reward: 382.7526820968189\n",
            "Total Timesteps: 67465 Episode Num: 98 Reward: 455.52053098677357\n",
            "Total Timesteps: 68465 Episode Num: 99 Reward: 538.6766104383108\n",
            "Total Timesteps: 69465 Episode Num: 100 Reward: 435.6570985347584\n",
            "Total Timesteps: 70465 Episode Num: 101 Reward: 420.2717465416228\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 467.154760\n",
            "---------------------------------------\n",
            "Total Timesteps: 71465 Episode Num: 102 Reward: 619.100607166048\n",
            "Total Timesteps: 72465 Episode Num: 103 Reward: 579.117937554279\n",
            "Total Timesteps: 73465 Episode Num: 104 Reward: 436.02569374512024\n",
            "Total Timesteps: 74465 Episode Num: 105 Reward: 669.6549512909668\n",
            "Total Timesteps: 75465 Episode Num: 106 Reward: 707.2470312357159\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 511.549905\n",
            "---------------------------------------\n",
            "Total Timesteps: 76465 Episode Num: 107 Reward: 578.2166988359006\n",
            "Total Timesteps: 77465 Episode Num: 108 Reward: 569.8825854225391\n",
            "Total Timesteps: 78465 Episode Num: 109 Reward: 462.2971404342152\n",
            "Total Timesteps: 79465 Episode Num: 110 Reward: 377.3052141333609\n",
            "Total Timesteps: 80465 Episode Num: 111 Reward: 617.8806270626511\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 478.495820\n",
            "---------------------------------------\n",
            "Total Timesteps: 81465 Episode Num: 112 Reward: 485.7243186850916\n",
            "Total Timesteps: 82465 Episode Num: 113 Reward: 428.48671076021134\n",
            "Total Timesteps: 83465 Episode Num: 114 Reward: 667.7530315102097\n",
            "Total Timesteps: 84465 Episode Num: 115 Reward: 561.4470835629263\n",
            "Total Timesteps: 85465 Episode Num: 116 Reward: 477.7181296115072\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 501.797841\n",
            "---------------------------------------\n",
            "Total Timesteps: 86465 Episode Num: 117 Reward: 482.20532181630887\n",
            "Total Timesteps: 87465 Episode Num: 118 Reward: 312.49649963957927\n",
            "Total Timesteps: 88465 Episode Num: 119 Reward: 175.4865459699056\n",
            "Total Timesteps: 89465 Episode Num: 120 Reward: 353.1554527115413\n",
            "Total Timesteps: 90465 Episode Num: 121 Reward: 460.974032759263\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 391.605691\n",
            "---------------------------------------\n",
            "Total Timesteps: 91465 Episode Num: 122 Reward: 520.2713290768525\n",
            "Total Timesteps: 92465 Episode Num: 123 Reward: 363.8402743254828\n",
            "Total Timesteps: 93465 Episode Num: 124 Reward: 206.34024289618395\n",
            "Total Timesteps: 94465 Episode Num: 125 Reward: 453.00264927683577\n",
            "Total Timesteps: 95465 Episode Num: 126 Reward: 685.117737932536\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 414.674177\n",
            "---------------------------------------\n",
            "Total Timesteps: 95736 Episode Num: 127 Reward: 157.80851539125155\n",
            "Total Timesteps: 96126 Episode Num: 128 Reward: 173.64009984902287\n",
            "Total Timesteps: 96989 Episode Num: 129 Reward: 404.1835409006488\n",
            "Total Timesteps: 97989 Episode Num: 130 Reward: 584.3578717723661\n",
            "Total Timesteps: 98989 Episode Num: 131 Reward: 437.4030275849206\n",
            "Total Timesteps: 99989 Episode Num: 132 Reward: 431.77256847594276\n",
            "Total Timesteps: 100989 Episode Num: 133 Reward: 546.9350392414566\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 437.219002\n",
            "---------------------------------------\n",
            "Total Timesteps: 101989 Episode Num: 134 Reward: 368.5748776836234\n",
            "Total Timesteps: 102989 Episode Num: 135 Reward: 434.2672004534483\n",
            "Total Timesteps: 103989 Episode Num: 136 Reward: 481.022667163086\n",
            "Total Timesteps: 104989 Episode Num: 137 Reward: 440.02163401024245\n",
            "Total Timesteps: 105989 Episode Num: 138 Reward: 345.255022148346\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 454.232408\n",
            "---------------------------------------\n",
            "Total Timesteps: 106989 Episode Num: 139 Reward: 516.973294627603\n",
            "Total Timesteps: 107989 Episode Num: 140 Reward: 360.2322423288469\n",
            "Total Timesteps: 108989 Episode Num: 141 Reward: 581.0801282311236\n",
            "Total Timesteps: 109989 Episode Num: 142 Reward: 491.2138596181478\n",
            "Total Timesteps: 110989 Episode Num: 143 Reward: 431.9203303918695\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 587.829278\n",
            "---------------------------------------\n",
            "Total Timesteps: 111989 Episode Num: 144 Reward: 504.67803910942723\n",
            "Total Timesteps: 112989 Episode Num: 145 Reward: 581.4602639344635\n",
            "Total Timesteps: 113989 Episode Num: 146 Reward: 393.6690058911451\n",
            "Total Timesteps: 114989 Episode Num: 147 Reward: 477.6829317574949\n",
            "Total Timesteps: 115989 Episode Num: 148 Reward: 447.2649579188923\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 467.653486\n",
            "---------------------------------------\n",
            "Total Timesteps: 116989 Episode Num: 149 Reward: 562.7151899806006\n",
            "Total Timesteps: 117989 Episode Num: 150 Reward: 634.0049938477569\n",
            "Total Timesteps: 118989 Episode Num: 151 Reward: 618.9400362489591\n",
            "Total Timesteps: 119989 Episode Num: 152 Reward: 554.459547563268\n",
            "Total Timesteps: 120035 Episode Num: 153 Reward: 25.79305337841355\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 268.564459\n",
            "---------------------------------------\n",
            "Total Timesteps: 120425 Episode Num: 154 Reward: 223.03095184976974\n",
            "Total Timesteps: 121425 Episode Num: 155 Reward: 429.49191526381827\n",
            "Total Timesteps: 122425 Episode Num: 156 Reward: 444.69016605321906\n",
            "Total Timesteps: 122716 Episode Num: 157 Reward: 149.125862830378\n",
            "Total Timesteps: 123716 Episode Num: 158 Reward: 573.9381145450313\n",
            "Total Timesteps: 124716 Episode Num: 159 Reward: 652.5258127658747\n",
            "Total Timesteps: 125716 Episode Num: 160 Reward: 484.31568606558073\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 39.117663\n",
            "---------------------------------------\n",
            "Total Timesteps: 125736 Episode Num: 161 Reward: 4.96599736848351\n",
            "Total Timesteps: 125758 Episode Num: 162 Reward: 8.077106098724242\n",
            "Total Timesteps: 125778 Episode Num: 163 Reward: 4.7943651074200995\n",
            "Total Timesteps: 125798 Episode Num: 164 Reward: 4.990011656060564\n",
            "Total Timesteps: 125818 Episode Num: 165 Reward: 4.093353268052672\n",
            "Total Timesteps: 125838 Episode Num: 166 Reward: 5.270303231243094\n",
            "Total Timesteps: 125858 Episode Num: 167 Reward: 5.603317624213525\n",
            "Total Timesteps: 125878 Episode Num: 168 Reward: 5.706727450425067\n",
            "Total Timesteps: 125898 Episode Num: 169 Reward: 5.748288798078205\n",
            "Total Timesteps: 125919 Episode Num: 170 Reward: 6.732185498913664\n",
            "Total Timesteps: 126693 Episode Num: 171 Reward: 428.0674423829928\n",
            "Total Timesteps: 127693 Episode Num: 172 Reward: 501.45767511681396\n",
            "Total Timesteps: 127713 Episode Num: 173 Reward: 8.193583009401149\n",
            "Total Timesteps: 128713 Episode Num: 174 Reward: 637.5040481117079\n",
            "Total Timesteps: 129713 Episode Num: 175 Reward: 515.4051141127052\n",
            "Total Timesteps: 130713 Episode Num: 176 Reward: 453.61884652349136\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 475.151569\n",
            "---------------------------------------\n",
            "Total Timesteps: 130735 Episode Num: 177 Reward: 7.404376211090998\n",
            "Total Timesteps: 131688 Episode Num: 178 Reward: 531.3604220118275\n",
            "Total Timesteps: 132060 Episode Num: 179 Reward: 193.85681973200371\n",
            "Total Timesteps: 133060 Episode Num: 180 Reward: 515.26199450274\n",
            "Total Timesteps: 133080 Episode Num: 181 Reward: 3.026893909733255\n",
            "Total Timesteps: 133100 Episode Num: 182 Reward: 2.9665805529225984\n",
            "Total Timesteps: 133120 Episode Num: 183 Reward: 2.911010686377117\n",
            "Total Timesteps: 133140 Episode Num: 184 Reward: 4.394050869568703\n",
            "Total Timesteps: 133160 Episode Num: 185 Reward: 4.553100558619789\n",
            "Total Timesteps: 133180 Episode Num: 186 Reward: 6.144028624049694\n",
            "Total Timesteps: 133200 Episode Num: 187 Reward: 4.726119313668978\n",
            "Total Timesteps: 133220 Episode Num: 188 Reward: 3.107264697740673\n",
            "Total Timesteps: 133240 Episode Num: 189 Reward: 2.771644859142996\n",
            "Total Timesteps: 133260 Episode Num: 190 Reward: 3.054137489373821\n",
            "Total Timesteps: 133280 Episode Num: 191 Reward: 1.954461649912512\n",
            "Total Timesteps: 133300 Episode Num: 192 Reward: 2.219235140509503\n",
            "Total Timesteps: 133320 Episode Num: 193 Reward: 1.6189515654260636\n",
            "Total Timesteps: 134320 Episode Num: 194 Reward: 565.0510842258064\n",
            "Total Timesteps: 134441 Episode Num: 195 Reward: 27.795927443148447\n",
            "Total Timesteps: 134461 Episode Num: 196 Reward: 2.590546951022096\n",
            "Total Timesteps: 134482 Episode Num: 197 Reward: 5.675043479541799\n",
            "Total Timesteps: 134503 Episode Num: 198 Reward: 3.5005700315405184\n",
            "Total Timesteps: 135503 Episode Num: 199 Reward: 233.70757769737241\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 523.154026\n",
            "---------------------------------------\n",
            "Total Timesteps: 136503 Episode Num: 200 Reward: 543.2429189044708\n",
            "Total Timesteps: 137503 Episode Num: 201 Reward: 530.1390778212561\n",
            "Total Timesteps: 138503 Episode Num: 202 Reward: 312.9650625512856\n",
            "Total Timesteps: 139503 Episode Num: 203 Reward: 423.7176218281247\n",
            "Total Timesteps: 140503 Episode Num: 204 Reward: 302.32574832181297\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 278.377194\n",
            "---------------------------------------\n",
            "Total Timesteps: 141503 Episode Num: 205 Reward: 231.95535399674685\n",
            "Total Timesteps: 142503 Episode Num: 206 Reward: 510.4024623550394\n",
            "Total Timesteps: 143503 Episode Num: 207 Reward: 603.8679071677727\n",
            "Total Timesteps: 144503 Episode Num: 208 Reward: 561.6080832493008\n",
            "Total Timesteps: 145503 Episode Num: 209 Reward: 580.3189254342628\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 516.388258\n",
            "---------------------------------------\n",
            "Total Timesteps: 146503 Episode Num: 210 Reward: 519.3296594317331\n",
            "Total Timesteps: 147503 Episode Num: 211 Reward: 548.0894350032108\n",
            "Total Timesteps: 148503 Episode Num: 212 Reward: 230.63316704961628\n",
            "Total Timesteps: 149503 Episode Num: 213 Reward: 507.11077093640574\n",
            "Total Timesteps: 150503 Episode Num: 214 Reward: 543.1159324550829\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 488.120279\n",
            "---------------------------------------\n",
            "Total Timesteps: 151503 Episode Num: 215 Reward: 631.3819604404217\n",
            "Total Timesteps: 152503 Episode Num: 216 Reward: 522.6109245230986\n",
            "Total Timesteps: 153503 Episode Num: 217 Reward: 615.5137888465806\n",
            "Total Timesteps: 154503 Episode Num: 218 Reward: 396.2259804168696\n",
            "Total Timesteps: 155503 Episode Num: 219 Reward: 619.4190903701867\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 534.839448\n",
            "---------------------------------------\n",
            "Total Timesteps: 156503 Episode Num: 220 Reward: 613.9126559471061\n",
            "Total Timesteps: 157503 Episode Num: 221 Reward: 626.0639731422533\n",
            "Total Timesteps: 158503 Episode Num: 222 Reward: 351.9850473733373\n",
            "Total Timesteps: 159503 Episode Num: 223 Reward: 432.44096755710956\n",
            "Total Timesteps: 160503 Episode Num: 224 Reward: 543.2511915452479\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 427.270070\n",
            "---------------------------------------\n",
            "Total Timesteps: 161503 Episode Num: 225 Reward: 246.15498048430044\n",
            "Total Timesteps: 162503 Episode Num: 226 Reward: 612.4313594785094\n",
            "Total Timesteps: 163503 Episode Num: 227 Reward: 518.4254148805583\n",
            "Total Timesteps: 164503 Episode Num: 228 Reward: 401.9345900936983\n",
            "Total Timesteps: 165503 Episode Num: 229 Reward: 289.0129042823941\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 426.940166\n",
            "---------------------------------------\n",
            "Total Timesteps: 166503 Episode Num: 230 Reward: 491.76784661276866\n",
            "Total Timesteps: 167503 Episode Num: 231 Reward: 300.8287461746916\n",
            "Total Timesteps: 168503 Episode Num: 232 Reward: 550.7348080738683\n",
            "Total Timesteps: 169503 Episode Num: 233 Reward: 330.0651953499379\n",
            "Total Timesteps: 170503 Episode Num: 234 Reward: 682.748063542158\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 457.280663\n",
            "---------------------------------------\n",
            "Total Timesteps: 171503 Episode Num: 235 Reward: 452.82446670931495\n",
            "Total Timesteps: 172503 Episode Num: 236 Reward: 721.3634182395803\n",
            "Total Timesteps: 173503 Episode Num: 237 Reward: 441.16827997706247\n",
            "Total Timesteps: 174503 Episode Num: 238 Reward: 464.442691311823\n",
            "Total Timesteps: 175503 Episode Num: 239 Reward: 725.646133948599\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 610.787589\n",
            "---------------------------------------\n",
            "Total Timesteps: 176503 Episode Num: 240 Reward: 547.623048367382\n",
            "Total Timesteps: 177503 Episode Num: 241 Reward: 568.2136398001895\n",
            "Total Timesteps: 178503 Episode Num: 242 Reward: 687.7720866275305\n",
            "Total Timesteps: 179503 Episode Num: 243 Reward: 512.9784142746591\n",
            "Total Timesteps: 180503 Episode Num: 244 Reward: 725.9672890519198\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 665.966497\n",
            "---------------------------------------\n",
            "Total Timesteps: 181503 Episode Num: 245 Reward: 592.562013481072\n",
            "Total Timesteps: 182503 Episode Num: 246 Reward: 640.0867111799165\n",
            "Total Timesteps: 183503 Episode Num: 247 Reward: 610.4480985031364\n",
            "Total Timesteps: 184503 Episode Num: 248 Reward: 393.2505887013876\n",
            "Total Timesteps: 185503 Episode Num: 249 Reward: 633.7698648097487\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 545.031074\n",
            "---------------------------------------\n",
            "Total Timesteps: 186503 Episode Num: 250 Reward: 506.55067075914843\n",
            "Total Timesteps: 187503 Episode Num: 251 Reward: 709.3286733359517\n",
            "Total Timesteps: 188503 Episode Num: 252 Reward: 585.9070252594621\n",
            "Total Timesteps: 189503 Episode Num: 253 Reward: 649.4005999845199\n",
            "Total Timesteps: 190503 Episode Num: 254 Reward: 731.2820578493678\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 657.609948\n",
            "---------------------------------------\n",
            "Total Timesteps: 191503 Episode Num: 255 Reward: 680.864823741189\n",
            "Total Timesteps: 192503 Episode Num: 256 Reward: 392.11751705194723\n",
            "Total Timesteps: 193503 Episode Num: 257 Reward: 566.8303792332093\n",
            "Total Timesteps: 194503 Episode Num: 258 Reward: 490.3666515074376\n",
            "Total Timesteps: 195503 Episode Num: 259 Reward: 569.0879264674254\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 370.748035\n",
            "---------------------------------------\n",
            "Total Timesteps: 196503 Episode Num: 260 Reward: 698.2959393740961\n",
            "Total Timesteps: 197503 Episode Num: 261 Reward: 461.19899430960305\n",
            "Total Timesteps: 198503 Episode Num: 262 Reward: 442.0867940020869\n",
            "Total Timesteps: 199503 Episode Num: 263 Reward: 684.7905139009334\n",
            "Total Timesteps: 200503 Episode Num: 264 Reward: 285.0430425783651\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 494.129990\n",
            "---------------------------------------\n",
            "Total Timesteps: 201503 Episode Num: 265 Reward: 525.1721316247824\n",
            "Total Timesteps: 202503 Episode Num: 266 Reward: 571.2613324892021\n",
            "Total Timesteps: 203503 Episode Num: 267 Reward: 613.8771756775305\n",
            "Total Timesteps: 204503 Episode Num: 268 Reward: 566.6785635055262\n",
            "Total Timesteps: 205503 Episode Num: 269 Reward: 464.6853183636297\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 651.396338\n",
            "---------------------------------------\n",
            "Total Timesteps: 206503 Episode Num: 270 Reward: 663.7615739271498\n",
            "Total Timesteps: 207503 Episode Num: 271 Reward: 737.2987826268951\n",
            "Total Timesteps: 208503 Episode Num: 272 Reward: 654.3287677158679\n",
            "Total Timesteps: 209503 Episode Num: 273 Reward: 582.9313265586871\n",
            "Total Timesteps: 210503 Episode Num: 274 Reward: 640.550311757961\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 665.709904\n",
            "---------------------------------------\n",
            "Total Timesteps: 211503 Episode Num: 275 Reward: 752.4048453515974\n",
            "Total Timesteps: 212503 Episode Num: 276 Reward: 585.8294360754751\n",
            "Total Timesteps: 213503 Episode Num: 277 Reward: 501.7518698726625\n",
            "Total Timesteps: 214503 Episode Num: 278 Reward: 504.0250495662792\n",
            "Total Timesteps: 215503 Episode Num: 279 Reward: 652.5724905940799\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 662.171865\n",
            "---------------------------------------\n",
            "Total Timesteps: 216503 Episode Num: 280 Reward: 641.4990952808394\n",
            "Total Timesteps: 217503 Episode Num: 281 Reward: 664.7662243749731\n",
            "Total Timesteps: 218503 Episode Num: 282 Reward: 678.6881434232699\n",
            "Total Timesteps: 219503 Episode Num: 283 Reward: 761.2669201962912\n",
            "Total Timesteps: 220503 Episode Num: 284 Reward: 708.5273089706295\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 724.551522\n",
            "---------------------------------------\n",
            "Total Timesteps: 221503 Episode Num: 285 Reward: 756.0087380226071\n",
            "Total Timesteps: 222503 Episode Num: 286 Reward: 656.9897242825691\n",
            "Total Timesteps: 223503 Episode Num: 287 Reward: 596.4297171712609\n",
            "Total Timesteps: 224503 Episode Num: 288 Reward: 668.3228448711004\n",
            "Total Timesteps: 225503 Episode Num: 289 Reward: 303.45783785001055\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 705.619822\n",
            "---------------------------------------\n",
            "Total Timesteps: 226503 Episode Num: 290 Reward: 933.6391359090125\n",
            "Total Timesteps: 227503 Episode Num: 291 Reward: 650.1167623475045\n",
            "Total Timesteps: 228503 Episode Num: 292 Reward: 552.5342189434359\n",
            "Total Timesteps: 229503 Episode Num: 293 Reward: 549.402214486739\n",
            "Total Timesteps: 230503 Episode Num: 294 Reward: 862.3867042957465\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 486.319863\n",
            "---------------------------------------\n",
            "Total Timesteps: 231503 Episode Num: 295 Reward: 409.477943717397\n",
            "Total Timesteps: 232503 Episode Num: 296 Reward: 667.233506777684\n",
            "Total Timesteps: 233503 Episode Num: 297 Reward: 690.2437642146294\n",
            "Total Timesteps: 234503 Episode Num: 298 Reward: 745.8118697481464\n",
            "Total Timesteps: 235503 Episode Num: 299 Reward: 751.5837801900415\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 615.038590\n",
            "---------------------------------------\n",
            "Total Timesteps: 236503 Episode Num: 300 Reward: 672.184160692165\n",
            "Total Timesteps: 237503 Episode Num: 301 Reward: 496.3354624692629\n",
            "Total Timesteps: 238503 Episode Num: 302 Reward: 661.923085750827\n",
            "Total Timesteps: 239503 Episode Num: 303 Reward: 594.9389898822823\n",
            "Total Timesteps: 240503 Episode Num: 304 Reward: 442.3724337059668\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 267.950057\n",
            "---------------------------------------\n",
            "Total Timesteps: 241503 Episode Num: 305 Reward: 321.847687753313\n",
            "Total Timesteps: 242503 Episode Num: 306 Reward: 350.54873867453995\n",
            "Total Timesteps: 243503 Episode Num: 307 Reward: 699.4634159443892\n",
            "Total Timesteps: 244503 Episode Num: 308 Reward: 427.72425724454683\n",
            "Total Timesteps: 245503 Episode Num: 309 Reward: 567.750384726325\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 613.339275\n",
            "---------------------------------------\n",
            "Total Timesteps: 246503 Episode Num: 310 Reward: 464.1805857010449\n",
            "Total Timesteps: 247503 Episode Num: 311 Reward: 656.7889995446716\n",
            "Total Timesteps: 248503 Episode Num: 312 Reward: 628.811320633639\n",
            "Total Timesteps: 249503 Episode Num: 313 Reward: 751.506254244322\n",
            "Total Timesteps: 250503 Episode Num: 314 Reward: 861.1850598682963\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 690.100518\n",
            "---------------------------------------\n",
            "Total Timesteps: 251503 Episode Num: 315 Reward: 626.5110043499001\n",
            "Total Timesteps: 252503 Episode Num: 316 Reward: 528.1646600534841\n",
            "Total Timesteps: 253503 Episode Num: 317 Reward: 565.1730332100449\n",
            "Total Timesteps: 254503 Episode Num: 318 Reward: 645.9377342612656\n",
            "Total Timesteps: 255503 Episode Num: 319 Reward: 649.6477114832971\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 595.706487\n",
            "---------------------------------------\n",
            "Total Timesteps: 256503 Episode Num: 320 Reward: 467.9090284963731\n",
            "Total Timesteps: 257503 Episode Num: 321 Reward: 551.5441442583852\n",
            "Total Timesteps: 258503 Episode Num: 322 Reward: 541.371713525607\n",
            "Total Timesteps: 259503 Episode Num: 323 Reward: 620.014443835139\n",
            "Total Timesteps: 260503 Episode Num: 324 Reward: 714.3615715349387\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 584.426563\n",
            "---------------------------------------\n",
            "Total Timesteps: 261503 Episode Num: 325 Reward: 731.9469959424074\n",
            "Total Timesteps: 262503 Episode Num: 326 Reward: 669.7310733441018\n",
            "Total Timesteps: 263503 Episode Num: 327 Reward: 545.6659868225822\n",
            "Total Timesteps: 264503 Episode Num: 328 Reward: 640.8930791705227\n",
            "Total Timesteps: 265503 Episode Num: 329 Reward: 690.8594343247544\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 606.992175\n",
            "---------------------------------------\n",
            "Total Timesteps: 266503 Episode Num: 330 Reward: 578.4618571328817\n",
            "Total Timesteps: 267503 Episode Num: 331 Reward: 529.1322655815927\n",
            "Total Timesteps: 268503 Episode Num: 332 Reward: 427.34553726056566\n",
            "Total Timesteps: 269503 Episode Num: 333 Reward: 450.7394415139379\n",
            "Total Timesteps: 270503 Episode Num: 334 Reward: 569.560000801492\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 591.811663\n",
            "---------------------------------------\n",
            "Total Timesteps: 271503 Episode Num: 335 Reward: 624.8996090416595\n",
            "Total Timesteps: 272503 Episode Num: 336 Reward: 550.8234852370704\n",
            "Total Timesteps: 273503 Episode Num: 337 Reward: 704.1003092669313\n",
            "Total Timesteps: 274503 Episode Num: 338 Reward: 668.2127974763873\n",
            "Total Timesteps: 275503 Episode Num: 339 Reward: 585.081846393277\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 599.906012\n",
            "---------------------------------------\n",
            "Total Timesteps: 276503 Episode Num: 340 Reward: 725.6200534432545\n",
            "Total Timesteps: 277503 Episode Num: 341 Reward: 653.2811618516034\n",
            "Total Timesteps: 278503 Episode Num: 342 Reward: 646.2074198590826\n",
            "Total Timesteps: 279503 Episode Num: 343 Reward: 708.2624357235268\n",
            "Total Timesteps: 280503 Episode Num: 344 Reward: 568.8456268856241\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 664.029694\n",
            "---------------------------------------\n",
            "Total Timesteps: 281503 Episode Num: 345 Reward: 655.1378515796199\n",
            "Total Timesteps: 282503 Episode Num: 346 Reward: 721.215260915948\n",
            "Total Timesteps: 283503 Episode Num: 347 Reward: 612.9234939946983\n",
            "Total Timesteps: 284503 Episode Num: 348 Reward: 424.7605960022582\n",
            "Total Timesteps: 285503 Episode Num: 349 Reward: 786.1555014539987\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 619.051835\n",
            "---------------------------------------\n",
            "Total Timesteps: 286503 Episode Num: 350 Reward: 578.8487271292864\n",
            "Total Timesteps: 287503 Episode Num: 351 Reward: 561.8361371163488\n",
            "Total Timesteps: 288503 Episode Num: 352 Reward: 420.61275508239316\n",
            "Total Timesteps: 289503 Episode Num: 353 Reward: 438.51788102989696\n",
            "Total Timesteps: 290503 Episode Num: 354 Reward: 431.4447292168758\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 683.971897\n",
            "---------------------------------------\n",
            "Total Timesteps: 291503 Episode Num: 355 Reward: 645.0024916860322\n",
            "Total Timesteps: 292503 Episode Num: 356 Reward: 556.3679940046497\n",
            "Total Timesteps: 293503 Episode Num: 357 Reward: 550.0835492559145\n",
            "Total Timesteps: 294503 Episode Num: 358 Reward: 777.9163663991193\n",
            "Total Timesteps: 295503 Episode Num: 359 Reward: 819.4464583824405\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 696.235491\n",
            "---------------------------------------\n",
            "Total Timesteps: 296503 Episode Num: 360 Reward: 860.2335604983728\n",
            "Total Timesteps: 297503 Episode Num: 361 Reward: 658.6461970638142\n",
            "Total Timesteps: 298503 Episode Num: 362 Reward: 794.9445092217283\n",
            "Total Timesteps: 299503 Episode Num: 363 Reward: 731.1851628969133\n",
            "Total Timesteps: 300503 Episode Num: 364 Reward: 664.8380344001886\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 678.060765\n",
            "---------------------------------------\n",
            "Total Timesteps: 301503 Episode Num: 365 Reward: 700.2220659186834\n",
            "Total Timesteps: 302503 Episode Num: 366 Reward: 488.6366957404284\n",
            "Total Timesteps: 303503 Episode Num: 367 Reward: 639.9439017732993\n",
            "Total Timesteps: 304503 Episode Num: 368 Reward: 641.1843980694189\n",
            "Total Timesteps: 305503 Episode Num: 369 Reward: 649.0565380656733\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 665.626970\n",
            "---------------------------------------\n",
            "Total Timesteps: 306503 Episode Num: 370 Reward: 618.9696886326007\n",
            "Total Timesteps: 307503 Episode Num: 371 Reward: 585.8446564812833\n",
            "Total Timesteps: 308503 Episode Num: 372 Reward: 507.78935098515575\n",
            "Total Timesteps: 309503 Episode Num: 373 Reward: 443.72720312798066\n",
            "Total Timesteps: 310503 Episode Num: 374 Reward: 677.3635635364354\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 657.565132\n",
            "---------------------------------------\n",
            "Total Timesteps: 311503 Episode Num: 375 Reward: 757.2055343908983\n",
            "Total Timesteps: 312503 Episode Num: 376 Reward: 689.2230781321339\n",
            "Total Timesteps: 313503 Episode Num: 377 Reward: 763.3790413338929\n",
            "Total Timesteps: 314503 Episode Num: 378 Reward: 770.557594738309\n",
            "Total Timesteps: 315503 Episode Num: 379 Reward: 769.2309651171306\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 734.461487\n",
            "---------------------------------------\n",
            "Total Timesteps: 316503 Episode Num: 380 Reward: 722.4239657020346\n",
            "Total Timesteps: 317503 Episode Num: 381 Reward: 790.9545517347452\n",
            "Total Timesteps: 318503 Episode Num: 382 Reward: 820.6126825586681\n",
            "Total Timesteps: 319503 Episode Num: 383 Reward: 649.5776810239616\n",
            "Total Timesteps: 320503 Episode Num: 384 Reward: 643.7188408222141\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 748.009557\n",
            "---------------------------------------\n",
            "Total Timesteps: 321503 Episode Num: 385 Reward: 839.6234751485384\n",
            "Total Timesteps: 322503 Episode Num: 386 Reward: 607.7961355700633\n",
            "Total Timesteps: 323503 Episode Num: 387 Reward: 678.057280494131\n",
            "Total Timesteps: 324503 Episode Num: 388 Reward: 737.3597832158953\n",
            "Total Timesteps: 325503 Episode Num: 389 Reward: 727.2686779225454\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 745.859835\n",
            "---------------------------------------\n",
            "Total Timesteps: 326503 Episode Num: 390 Reward: 822.3019217995937\n",
            "Total Timesteps: 327503 Episode Num: 391 Reward: 655.3706628771632\n",
            "Total Timesteps: 328503 Episode Num: 392 Reward: 695.7485394240649\n",
            "Total Timesteps: 329503 Episode Num: 393 Reward: 734.0005644761053\n",
            "Total Timesteps: 330503 Episode Num: 394 Reward: 613.0791592492227\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 756.532995\n",
            "---------------------------------------\n",
            "Total Timesteps: 331503 Episode Num: 395 Reward: 774.0100369422341\n",
            "Total Timesteps: 332503 Episode Num: 396 Reward: 777.4284908944267\n",
            "Total Timesteps: 333503 Episode Num: 397 Reward: 674.2086496465283\n",
            "Total Timesteps: 334503 Episode Num: 398 Reward: 764.9795743696383\n",
            "Total Timesteps: 335503 Episode Num: 399 Reward: 846.9059110108246\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 745.497031\n",
            "---------------------------------------\n",
            "Total Timesteps: 336503 Episode Num: 400 Reward: 774.2518174646459\n",
            "Total Timesteps: 337503 Episode Num: 401 Reward: 717.9844109685104\n",
            "Total Timesteps: 338503 Episode Num: 402 Reward: 692.3916344620231\n",
            "Total Timesteps: 339503 Episode Num: 403 Reward: 658.3029962194979\n",
            "Total Timesteps: 340503 Episode Num: 404 Reward: 681.4753281762769\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 695.896689\n",
            "---------------------------------------\n",
            "Total Timesteps: 341503 Episode Num: 405 Reward: 743.9158235721675\n",
            "Total Timesteps: 342503 Episode Num: 406 Reward: 642.4468073441137\n",
            "Total Timesteps: 343503 Episode Num: 407 Reward: 805.6937944387945\n",
            "Total Timesteps: 344503 Episode Num: 408 Reward: 573.7709191391423\n",
            "Total Timesteps: 345503 Episode Num: 409 Reward: 641.6213455338105\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 693.285331\n",
            "---------------------------------------\n",
            "Total Timesteps: 346503 Episode Num: 410 Reward: 527.6356825691283\n",
            "Total Timesteps: 347503 Episode Num: 411 Reward: 709.629825132429\n",
            "Total Timesteps: 348503 Episode Num: 412 Reward: 560.4113843290334\n",
            "Total Timesteps: 349503 Episode Num: 413 Reward: 600.8180421472152\n",
            "Total Timesteps: 350503 Episode Num: 414 Reward: 659.2405270302719\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 594.886823\n",
            "---------------------------------------\n",
            "Total Timesteps: 351503 Episode Num: 415 Reward: 689.8009382449258\n",
            "Total Timesteps: 352503 Episode Num: 416 Reward: 645.2923195601168\n",
            "Total Timesteps: 353503 Episode Num: 417 Reward: 464.6184111219502\n",
            "Total Timesteps: 354503 Episode Num: 418 Reward: 494.7034031027552\n",
            "Total Timesteps: 355503 Episode Num: 419 Reward: 577.3632865759326\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 695.914302\n",
            "---------------------------------------\n",
            "Total Timesteps: 356503 Episode Num: 420 Reward: 709.7047471496711\n",
            "Total Timesteps: 357503 Episode Num: 421 Reward: 845.7630130286146\n",
            "Total Timesteps: 358503 Episode Num: 422 Reward: 545.7283717898014\n",
            "Total Timesteps: 359503 Episode Num: 423 Reward: 542.7765414475437\n",
            "Total Timesteps: 360503 Episode Num: 424 Reward: 582.0536673709963\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 688.739875\n",
            "---------------------------------------\n",
            "Total Timesteps: 361503 Episode Num: 425 Reward: 649.7060772238647\n",
            "Total Timesteps: 362503 Episode Num: 426 Reward: 793.3967372824264\n",
            "Total Timesteps: 363503 Episode Num: 427 Reward: 1056.5446794836728\n",
            "Total Timesteps: 364503 Episode Num: 428 Reward: 664.9558817169938\n",
            "Total Timesteps: 365503 Episode Num: 429 Reward: 992.1150214988826\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 694.960908\n",
            "---------------------------------------\n",
            "Total Timesteps: 366503 Episode Num: 430 Reward: 525.0143092460435\n",
            "Total Timesteps: 367503 Episode Num: 431 Reward: 660.9671538434184\n",
            "Total Timesteps: 368503 Episode Num: 432 Reward: 742.371301327519\n",
            "Total Timesteps: 369503 Episode Num: 433 Reward: 692.1900803166354\n",
            "Total Timesteps: 370503 Episode Num: 434 Reward: 764.0588399840753\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 753.525504\n",
            "---------------------------------------\n",
            "Total Timesteps: 371503 Episode Num: 435 Reward: 743.3689991009284\n",
            "Total Timesteps: 372503 Episode Num: 436 Reward: 809.0581645379307\n",
            "Total Timesteps: 373503 Episode Num: 437 Reward: 897.1183016652502\n",
            "Total Timesteps: 374503 Episode Num: 438 Reward: 921.9642749011107\n",
            "Total Timesteps: 375503 Episode Num: 439 Reward: 903.8682151425801\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 776.266951\n",
            "---------------------------------------\n",
            "Total Timesteps: 376503 Episode Num: 440 Reward: 1128.8764919565365\n",
            "Total Timesteps: 377503 Episode Num: 441 Reward: 838.4419130479265\n",
            "Total Timesteps: 378503 Episode Num: 442 Reward: 373.56170487349704\n",
            "Total Timesteps: 379503 Episode Num: 443 Reward: 857.1368036863869\n",
            "Total Timesteps: 380503 Episode Num: 444 Reward: 741.9635701831693\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 785.935083\n",
            "---------------------------------------\n",
            "Total Timesteps: 381503 Episode Num: 445 Reward: 893.844157490737\n",
            "Total Timesteps: 382503 Episode Num: 446 Reward: 1212.0769099375946\n",
            "Total Timesteps: 383503 Episode Num: 447 Reward: 1007.3938841650782\n",
            "Total Timesteps: 384503 Episode Num: 448 Reward: 1214.719511743875\n",
            "Total Timesteps: 385503 Episode Num: 449 Reward: 1177.1476586603956\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1090.369769\n",
            "---------------------------------------\n",
            "Total Timesteps: 386503 Episode Num: 450 Reward: 1079.1984943008351\n",
            "Total Timesteps: 387503 Episode Num: 451 Reward: 781.2910738257533\n",
            "Total Timesteps: 388503 Episode Num: 452 Reward: 884.4372547147352\n",
            "Total Timesteps: 389503 Episode Num: 453 Reward: 1337.7816573346965\n",
            "Total Timesteps: 390503 Episode Num: 454 Reward: 1386.1938204984447\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 971.317936\n",
            "---------------------------------------\n",
            "Total Timesteps: 391503 Episode Num: 455 Reward: 814.9751251062945\n",
            "Total Timesteps: 392503 Episode Num: 456 Reward: 1267.712476684534\n",
            "Total Timesteps: 393503 Episode Num: 457 Reward: 1331.5950831812597\n",
            "Total Timesteps: 394503 Episode Num: 458 Reward: 1253.1239119988488\n",
            "Total Timesteps: 395503 Episode Num: 459 Reward: 1400.5977420243737\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1356.294904\n",
            "---------------------------------------\n",
            "Total Timesteps: 396503 Episode Num: 460 Reward: 1347.140573670769\n",
            "Total Timesteps: 397503 Episode Num: 461 Reward: 668.9666704661792\n",
            "Total Timesteps: 398503 Episode Num: 462 Reward: 1149.9894314952423\n",
            "Total Timesteps: 399503 Episode Num: 463 Reward: 1132.6424803543543\n",
            "Total Timesteps: 400503 Episode Num: 464 Reward: 1074.8567528845952\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 917.850584\n",
            "---------------------------------------\n",
            "Total Timesteps: 401503 Episode Num: 465 Reward: 867.3161582798162\n",
            "Total Timesteps: 402503 Episode Num: 466 Reward: 1312.2944027406447\n",
            "Total Timesteps: 403503 Episode Num: 467 Reward: 1318.7380687879356\n",
            "Total Timesteps: 404503 Episode Num: 468 Reward: 1426.1949877016277\n",
            "Total Timesteps: 405503 Episode Num: 469 Reward: 954.1714073380738\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1047.992950\n",
            "---------------------------------------\n",
            "Total Timesteps: 406503 Episode Num: 470 Reward: 1004.2278294603652\n",
            "Total Timesteps: 407503 Episode Num: 471 Reward: 1309.94690352258\n",
            "Total Timesteps: 408503 Episode Num: 472 Reward: 1216.8918424135127\n",
            "Total Timesteps: 409503 Episode Num: 473 Reward: 1298.4175626256922\n",
            "Total Timesteps: 410503 Episode Num: 474 Reward: 853.4470220520493\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1223.773663\n",
            "---------------------------------------\n",
            "Total Timesteps: 411503 Episode Num: 475 Reward: 1266.1028748340432\n",
            "Total Timesteps: 412503 Episode Num: 476 Reward: 1333.9304684578024\n",
            "Total Timesteps: 413503 Episode Num: 477 Reward: 1296.250822517667\n",
            "Total Timesteps: 414503 Episode Num: 478 Reward: 1293.7030368370454\n",
            "Total Timesteps: 415503 Episode Num: 479 Reward: 1306.194059546045\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1285.665057\n",
            "---------------------------------------\n",
            "Total Timesteps: 416503 Episode Num: 480 Reward: 1338.909498953778\n",
            "Total Timesteps: 417503 Episode Num: 481 Reward: 1309.130151111423\n",
            "Total Timesteps: 418503 Episode Num: 482 Reward: 1433.586048202501\n",
            "Total Timesteps: 419503 Episode Num: 483 Reward: 1347.0862199260655\n",
            "Total Timesteps: 420503 Episode Num: 484 Reward: 1416.1546105028942\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1390.927776\n",
            "---------------------------------------\n",
            "Total Timesteps: 421503 Episode Num: 485 Reward: 1442.722284733126\n",
            "Total Timesteps: 422503 Episode Num: 486 Reward: 1077.326297325731\n",
            "Total Timesteps: 423503 Episode Num: 487 Reward: 1208.741348660598\n",
            "Total Timesteps: 424503 Episode Num: 488 Reward: 1359.076946888239\n",
            "Total Timesteps: 425503 Episode Num: 489 Reward: 1339.1780183356916\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1377.815748\n",
            "---------------------------------------\n",
            "Total Timesteps: 426503 Episode Num: 490 Reward: 1398.2254777812739\n",
            "Total Timesteps: 427503 Episode Num: 491 Reward: 1273.1987474975247\n",
            "Total Timesteps: 428503 Episode Num: 492 Reward: 474.86769887398003\n",
            "Total Timesteps: 429503 Episode Num: 493 Reward: 1410.532865831984\n",
            "Total Timesteps: 430503 Episode Num: 494 Reward: 876.9554581248342\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1404.903697\n",
            "---------------------------------------\n",
            "Total Timesteps: 431503 Episode Num: 495 Reward: 1413.9019426027442\n",
            "Total Timesteps: 432503 Episode Num: 496 Reward: 1346.6819232679297\n",
            "Total Timesteps: 433503 Episode Num: 497 Reward: 1301.7439476419534\n",
            "Total Timesteps: 434503 Episode Num: 498 Reward: 1387.200669360789\n",
            "Total Timesteps: 435503 Episode Num: 499 Reward: 1394.0393204862933\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1422.752469\n",
            "---------------------------------------\n",
            "Total Timesteps: 436503 Episode Num: 500 Reward: 1444.256779619859\n",
            "Total Timesteps: 437503 Episode Num: 501 Reward: 1395.76558981214\n",
            "Total Timesteps: 438503 Episode Num: 502 Reward: 1396.5597400212566\n",
            "Total Timesteps: 439503 Episode Num: 503 Reward: 1335.3914280731985\n",
            "Total Timesteps: 440503 Episode Num: 504 Reward: 1444.535660154172\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1371.243463\n",
            "---------------------------------------\n",
            "Total Timesteps: 441503 Episode Num: 505 Reward: 1432.7424358893227\n",
            "Total Timesteps: 442503 Episode Num: 506 Reward: 1410.330290530116\n",
            "Total Timesteps: 443503 Episode Num: 507 Reward: 1429.9429871501738\n",
            "Total Timesteps: 444503 Episode Num: 508 Reward: 1482.16295060835\n",
            "Total Timesteps: 445503 Episode Num: 509 Reward: 1560.0451771401292\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1467.210841\n",
            "---------------------------------------\n",
            "Total Timesteps: 446503 Episode Num: 510 Reward: 1466.7131617803827\n",
            "Total Timesteps: 447503 Episode Num: 511 Reward: 1457.042802950636\n",
            "Total Timesteps: 448503 Episode Num: 512 Reward: 1505.4953361769246\n",
            "Total Timesteps: 449503 Episode Num: 513 Reward: 1580.963162571667\n",
            "Total Timesteps: 450503 Episode Num: 514 Reward: 1547.9176017657333\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1504.744261\n",
            "---------------------------------------\n",
            "Total Timesteps: 451503 Episode Num: 515 Reward: 1465.3250000496016\n",
            "Total Timesteps: 452503 Episode Num: 516 Reward: 1478.293655999348\n",
            "Total Timesteps: 453503 Episode Num: 517 Reward: 1496.698635471313\n",
            "Total Timesteps: 454503 Episode Num: 518 Reward: 1460.142216134244\n",
            "Total Timesteps: 455503 Episode Num: 519 Reward: 1486.568517992286\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1555.073198\n",
            "---------------------------------------\n",
            "Total Timesteps: 456503 Episode Num: 520 Reward: 1503.6255050436275\n",
            "Total Timesteps: 457503 Episode Num: 521 Reward: 1471.6516253773602\n",
            "Total Timesteps: 458503 Episode Num: 522 Reward: 1534.3324543841977\n",
            "Total Timesteps: 459503 Episode Num: 523 Reward: 1542.1221593849302\n",
            "Total Timesteps: 460503 Episode Num: 524 Reward: 1615.4121297967909\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1554.147412\n",
            "---------------------------------------\n",
            "Total Timesteps: 461503 Episode Num: 525 Reward: 1542.9801036398044\n",
            "Total Timesteps: 462503 Episode Num: 526 Reward: 1557.671343967936\n",
            "Total Timesteps: 463503 Episode Num: 527 Reward: 1528.3377096407496\n",
            "Total Timesteps: 464503 Episode Num: 528 Reward: 1573.324834271327\n",
            "Total Timesteps: 465503 Episode Num: 529 Reward: 1471.8496491327248\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1546.273332\n",
            "---------------------------------------\n",
            "Total Timesteps: 466503 Episode Num: 530 Reward: 1539.2156116318508\n",
            "Total Timesteps: 467503 Episode Num: 531 Reward: 1571.3188873229783\n",
            "Total Timesteps: 468503 Episode Num: 532 Reward: 1540.5831398886228\n",
            "Total Timesteps: 469503 Episode Num: 533 Reward: 1542.2395786901227\n",
            "Total Timesteps: 470503 Episode Num: 534 Reward: 1532.4699458707025\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1584.303644\n",
            "---------------------------------------\n",
            "Total Timesteps: 471503 Episode Num: 535 Reward: 1502.1464380408036\n",
            "Total Timesteps: 472503 Episode Num: 536 Reward: 1595.4345589077318\n",
            "Total Timesteps: 473503 Episode Num: 537 Reward: 1567.6662269228016\n",
            "Total Timesteps: 474503 Episode Num: 538 Reward: 1453.4658021369487\n",
            "Total Timesteps: 475503 Episode Num: 539 Reward: 1472.1938562765956\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1516.160340\n",
            "---------------------------------------\n",
            "Total Timesteps: 476503 Episode Num: 540 Reward: 1515.1380502789182\n",
            "Total Timesteps: 477503 Episode Num: 541 Reward: 1453.152848403045\n",
            "Total Timesteps: 478503 Episode Num: 542 Reward: 1455.5683097677806\n",
            "Total Timesteps: 479503 Episode Num: 543 Reward: 1458.0881782160009\n",
            "Total Timesteps: 480503 Episode Num: 544 Reward: 1440.8331742531946\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1657.909090\n",
            "---------------------------------------\n",
            "Total Timesteps: 481503 Episode Num: 545 Reward: 1595.4456573273142\n",
            "Total Timesteps: 482503 Episode Num: 546 Reward: 1534.0692035230738\n",
            "Total Timesteps: 483503 Episode Num: 547 Reward: 1596.2569046945214\n",
            "Total Timesteps: 484503 Episode Num: 548 Reward: 1613.9221435717834\n",
            "Total Timesteps: 485503 Episode Num: 549 Reward: 1642.688028413895\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1379.572722\n",
            "---------------------------------------\n",
            "Total Timesteps: 486503 Episode Num: 550 Reward: 1390.4819086080636\n",
            "Total Timesteps: 487503 Episode Num: 551 Reward: 1518.4270020949414\n",
            "Total Timesteps: 488503 Episode Num: 552 Reward: 1413.4153119683467\n",
            "Total Timesteps: 489503 Episode Num: 553 Reward: 1560.7890758269073\n",
            "Total Timesteps: 490503 Episode Num: 554 Reward: 1519.6895682288834\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1462.454483\n",
            "---------------------------------------\n",
            "Total Timesteps: 491503 Episode Num: 555 Reward: 1460.9883916194492\n",
            "Total Timesteps: 492503 Episode Num: 556 Reward: 1494.5836897302759\n",
            "Total Timesteps: 493503 Episode Num: 557 Reward: 1469.5163603956933\n",
            "Total Timesteps: 494503 Episode Num: 558 Reward: 1451.6852544767398\n",
            "Total Timesteps: 495503 Episode Num: 559 Reward: 1545.4721732754672\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1498.960087\n",
            "---------------------------------------\n",
            "Total Timesteps: 496503 Episode Num: 560 Reward: 1505.3824551872463\n",
            "Total Timesteps: 497503 Episode Num: 561 Reward: 1527.2362716428654\n",
            "Total Timesteps: 498503 Episode Num: 562 Reward: 1461.3468577982844\n",
            "Total Timesteps: 499503 Episode Num: 563 Reward: 1420.3852857138154\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1499.967055\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi6e2-_pu05e",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW4d1YAMqif1",
        "colab_type": "code",
        "outputId": "f6d0fe68-3f19-485d-c027-b56574cb3339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
        "    return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1\n",
        "\n",
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward\n",
        "\n",
        "env_name = \"AntBulletEnv-v0\"\n",
        "seed = 0\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "eval_episodes = 10\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "max_episode_steps = env._max_episode_steps\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1521.664823\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcnexWrW4a8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
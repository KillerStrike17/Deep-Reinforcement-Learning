{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement learning- Half Cheetah Bullet Environment .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KillerStrike17/Deep-Reinforcement-Learning/blob/master/TD3-Car-Environment/Reinforcement_learning_Half_Cheetah_Bullet_Environment_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXu1r8qvSzWf",
        "colab_type": "text"
      },
      "source": [
        "# Twin-Delayed DDPG\n",
        "\n",
        "Complete credit goes to this [awesome Deep Reinforcement Learning 2.0 Course on Udemy](https://www.udemy.com/course/deep-reinforcement-learning/) for the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRzQUhuUTc0J",
        "colab_type": "text"
      },
      "source": [
        "## Installing the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elq8muJOjfbQ",
        "colab_type": "code",
        "outputId": "f242bd38-a967-40b5-84b0-71b9add00537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Apr 19 16:34:38 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAHMB0Ze8fU0",
        "colab_type": "code",
        "outputId": "46d5b3e1-87b1-4b27-96ec-e77c4d18cc62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!pip install pybullet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pybullet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/c3/3b9bbc274ce7b056c3b01c1412779434dfe1cba200d44ffb1cf4fa5ba08c/pybullet-2.7.3-cp36-cp36m-manylinux1_x86_64.whl (95.0MB)\n",
            "\u001b[K     |████████████████████████████████| 95.0MB 54kB/s \n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-2.7.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjm2onHdT-Av",
        "colab_type": "text"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikr2p0Js8iB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nGdtlKVydr",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: We initialize the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5rW0IDB8nTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind: \n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb7TTaHxWbQD",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CeRW4D79HL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRDDce8FXef7",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCee7gwR9Jrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzIDuONodenW",
        "colab_type": "text"
      },
      "source": [
        "## Steps 4 to 15: Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zzd0H1xukdKe",
        "colab": {}
      },
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka-ZRtQvjBex",
        "colab_type": "text"
      },
      "source": [
        "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qabqiYdp9wDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGuKmH_ijf7U",
        "colab_type": "text"
      },
      "source": [
        "## We set the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFj6wbAo97lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_name = \"HalfCheetahBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjwf2HCol3XP",
        "colab_type": "text"
      },
      "source": [
        "## We create a file name for the two saved models: the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fyH8N5z-o3o",
        "colab_type": "code",
        "outputId": "5d36cb0e-7f4a-462a-bdc7-eb874e226a4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_HalfCheetahBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kop-C96Aml8O",
        "colab_type": "text"
      },
      "source": [
        "## We create a folder inside which will be saved the trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Src07lvY-zXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEAzOd47mv1Z",
        "colab_type": "text"
      },
      "source": [
        "## We create the PyBullet environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyQXJUIs-6BV",
        "colab_type": "code",
        "outputId": "1e252792-7209-4153-9d18-a73e88f21a5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "env = gym.make(env_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YdPG4HXnNsh",
        "colab_type": "text"
      },
      "source": [
        "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3RufYec_ADj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2NdvYgo4urn",
        "colab_type": "code",
        "outputId": "86dc0d07-0717-494b-bb64-baedfa80154f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "print(\"state dim: \",state_dim)\n",
        "print(\"State: \",env.observation_space.sample())\n",
        "print(\"State: \",dir(env.observation_space))\n",
        "print(\"State shape: \",env.observation_space.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "state dim:  26\n",
            "State:  [ 0.38937718  0.66008097 -0.20536721 -1.4338015  -1.2486714  -0.33020976\n",
            " -0.98876196 -0.60675657  1.7025404   1.3579198  -2.126984    0.4064805\n",
            " -1.2259623   0.3009699  -0.0462097   0.06594031  0.01704927  0.6692315\n",
            "  0.9922822   0.18301885 -0.91760755  0.7332102   0.86905736 -2.6112778\n",
            "  0.64177024  1.0075977 ]\n",
            "State:  ['__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'bounded_above', 'bounded_below', 'contains', 'dtype', 'from_jsonable', 'high', 'is_bounded', 'low', 'np_random', 'sample', 'seed', 'shape', 'to_jsonable']\n",
            "State shape:  (26,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBFWoL-m41TB",
        "colab_type": "code",
        "outputId": "9267900b-bb1e-435c-a1a7-797a58113416",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "print(\"Action_dim:\",action_dim)\n",
        "print(\"Action:\",env.action_space.sample())\n",
        "print(\"State: \",dir(env.action_space))\n",
        "print(\"State shape: \",env.action_space.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action_dim: 6\n",
            "Action: [-0.7252776   0.85720503 -0.25275385  0.7490303   0.73323715 -0.34701887]\n",
            "State:  ['__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'bounded_above', 'bounded_below', 'contains', 'dtype', 'from_jsonable', 'high', 'is_bounded', 'low', 'np_random', 'sample', 'seed', 'shape', 'to_jsonable']\n",
            "State shape:  (6,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7sHJkVF44M9",
        "colab_type": "code",
        "outputId": "3073fb1c-24b2-4cc5-fef4-856d6c221ef2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "print(\"Max_action :\",max_action)\n",
        "print(\"Max_action:\",env.action_space.high[0])\n",
        "print(\"dir: \",dir(env.action_space.high))\n",
        "print(\"Max_action: \",env.action_space.high.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max_action : 1.0\n",
            "Max_action: 1.0\n",
            "dir:  ['T', '__abs__', '__add__', '__and__', '__array__', '__array_finalize__', '__array_function__', '__array_interface__', '__array_prepare__', '__array_priority__', '__array_struct__', '__array_ufunc__', '__array_wrap__', '__bool__', '__class__', '__complex__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__delitem__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__iand__', '__ifloordiv__', '__ilshift__', '__imatmul__', '__imod__', '__imul__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__ior__', '__ipow__', '__irshift__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__lshift__', '__lt__', '__matmul__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmatmul__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__xor__', 'all', 'any', 'argmax', 'argmin', 'argpartition', 'argsort', 'astype', 'base', 'byteswap', 'choose', 'clip', 'compress', 'conj', 'conjugate', 'copy', 'ctypes', 'cumprod', 'cumsum', 'data', 'diagonal', 'dot', 'dtype', 'dump', 'dumps', 'fill', 'flags', 'flat', 'flatten', 'getfield', 'imag', 'item', 'itemset', 'itemsize', 'max', 'mean', 'min', 'nbytes', 'ndim', 'newbyteorder', 'nonzero', 'partition', 'prod', 'ptp', 'put', 'ravel', 'real', 'repeat', 'reshape', 'resize', 'round', 'searchsorted', 'setfield', 'setflags', 'shape', 'size', 'sort', 'squeeze', 'std', 'strides', 'sum', 'swapaxes', 'take', 'tobytes', 'tofile', 'tolist', 'tostring', 'trace', 'transpose', 'var', 'view']\n",
            "Max_action:  (6,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWEgDAQxnbem",
        "colab_type": "text"
      },
      "source": [
        "## We create the policy network (the Actor model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTVvG7F8_EWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI60VN2Unklh",
        "colab_type": "text"
      },
      "source": [
        "[link text](https://)## We create the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd-ZsdXR_LgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replay_buffer = ReplayBuffer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOpCyiDnw7s",
        "colab_type": "text"
      },
      "source": [
        "## We define a list where all the evaluation results over 10 episodes are stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhC_5XJ__Orp",
        "colab_type": "code",
        "outputId": "1657da8f-b439-4f0a-c57b-dc89243e8154",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "evaluations = [evaluate_policy(policy)]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1429.426662\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm-4b3p6rglE",
        "colab_type": "text"
      },
      "source": [
        "## We create a new folder directory in which the final results (videos of the agent) will be populated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTL9uMd0ru03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31n5eb03p-Fm",
        "colab_type": "text"
      },
      "source": [
        "## We initialize the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vN5EvxK_QhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9gsjvtPqLgT",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_ouY4NH_Y0I",
        "colab_type": "code",
        "outputId": "711e4c5c-b19f-4efa-c73b-529f82d3a774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "max_timesteps = 500000\n",
        "# We start the main loop over 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "  \n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "  \n",
        "  # We check if the episode is done\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  \n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "  \n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 1000 Episode Num: 1 Reward: -1192.0841984748183\n",
            "Total Timesteps: 2000 Episode Num: 2 Reward: -1145.6958799199347\n",
            "Total Timesteps: 3000 Episode Num: 3 Reward: -1127.2490867845183\n",
            "Total Timesteps: 4000 Episode Num: 4 Reward: -1067.0383090248827\n",
            "Total Timesteps: 5000 Episode Num: 5 Reward: -1186.544962123479\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 117.792146\n",
            "---------------------------------------\n",
            "Total Timesteps: 6000 Episode Num: 6 Reward: -1063.1061308636645\n",
            "Total Timesteps: 7000 Episode Num: 7 Reward: -1165.2126045853756\n",
            "Total Timesteps: 8000 Episode Num: 8 Reward: -1414.7915295642672\n",
            "Total Timesteps: 9000 Episode Num: 9 Reward: -1136.7101279814349\n",
            "Total Timesteps: 10000 Episode Num: 10 Reward: -1461.3152721724257\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1617.728166\n",
            "---------------------------------------\n",
            "Total Timesteps: 11000 Episode Num: 11 Reward: -1624.8284904874424\n",
            "Total Timesteps: 12000 Episode Num: 12 Reward: -729.6289911786869\n",
            "Total Timesteps: 13000 Episode Num: 13 Reward: -1175.3379250812193\n",
            "Total Timesteps: 14000 Episode Num: 14 Reward: -1291.7462571810627\n",
            "Total Timesteps: 15000 Episode Num: 15 Reward: -401.0370521046349\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1303.221638\n",
            "---------------------------------------\n",
            "Total Timesteps: 16000 Episode Num: 16 Reward: -1331.9808596197286\n",
            "Total Timesteps: 17000 Episode Num: 17 Reward: -1487.6281356760305\n",
            "Total Timesteps: 18000 Episode Num: 18 Reward: -482.8458700894451\n",
            "Total Timesteps: 19000 Episode Num: 19 Reward: 195.2147651925405\n",
            "Total Timesteps: 20000 Episode Num: 20 Reward: -1223.9333845360463\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -82.339314\n",
            "---------------------------------------\n",
            "Total Timesteps: 21000 Episode Num: 21 Reward: -95.10386848066844\n",
            "Total Timesteps: 22000 Episode Num: 22 Reward: 154.45784669032452\n",
            "Total Timesteps: 23000 Episode Num: 23 Reward: -23.377945909411793\n",
            "Total Timesteps: 24000 Episode Num: 24 Reward: 45.6155110492904\n",
            "Total Timesteps: 25000 Episode Num: 25 Reward: -189.5068833783641\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1692.620198\n",
            "---------------------------------------\n",
            "Total Timesteps: 26000 Episode Num: 26 Reward: -1655.8264083976076\n",
            "Total Timesteps: 27000 Episode Num: 27 Reward: -1644.337784796426\n",
            "Total Timesteps: 28000 Episode Num: 28 Reward: -1558.9918928937605\n",
            "Total Timesteps: 29000 Episode Num: 29 Reward: -1786.1450453744956\n",
            "Total Timesteps: 30000 Episode Num: 30 Reward: -2004.1562739569347\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1928.396287\n",
            "---------------------------------------\n",
            "Total Timesteps: 31000 Episode Num: 31 Reward: -2150.034669721038\n",
            "Total Timesteps: 32000 Episode Num: 32 Reward: -2212.0129214789285\n",
            "Total Timesteps: 33000 Episode Num: 33 Reward: -495.623729110305\n",
            "Total Timesteps: 34000 Episode Num: 34 Reward: -914.9393910120707\n",
            "Total Timesteps: 35000 Episode Num: 35 Reward: -1409.1914547626618\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1626.969534\n",
            "---------------------------------------\n",
            "Total Timesteps: 36000 Episode Num: 36 Reward: -1618.7311357762553\n",
            "Total Timesteps: 37000 Episode Num: 37 Reward: -1253.0109127825983\n",
            "Total Timesteps: 38000 Episode Num: 38 Reward: -1423.5329063073877\n",
            "Total Timesteps: 39000 Episode Num: 39 Reward: -1342.8551495405359\n",
            "Total Timesteps: 40000 Episode Num: 40 Reward: 212.61652662377645\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 129.405177\n",
            "---------------------------------------\n",
            "Total Timesteps: 41000 Episode Num: 41 Reward: 276.7212814887751\n",
            "Total Timesteps: 42000 Episode Num: 42 Reward: 305.1155763201966\n",
            "Total Timesteps: 43000 Episode Num: 43 Reward: 444.63999524610927\n",
            "Total Timesteps: 44000 Episode Num: 44 Reward: 350.08059022106755\n",
            "Total Timesteps: 45000 Episode Num: 45 Reward: 68.32686782886555\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 200.619476\n",
            "---------------------------------------\n",
            "Total Timesteps: 46000 Episode Num: 46 Reward: 430.51179245833544\n",
            "Total Timesteps: 47000 Episode Num: 47 Reward: -423.3642440520537\n",
            "Total Timesteps: 48000 Episode Num: 48 Reward: 413.8705959938046\n",
            "Total Timesteps: 49000 Episode Num: 49 Reward: 478.77984201556626\n",
            "Total Timesteps: 50000 Episode Num: 50 Reward: -1301.5599288071933\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 103.470000\n",
            "---------------------------------------\n",
            "Total Timesteps: 51000 Episode Num: 51 Reward: 503.55066272872557\n",
            "Total Timesteps: 52000 Episode Num: 52 Reward: -790.1786577621768\n",
            "Total Timesteps: 53000 Episode Num: 53 Reward: 237.5965303510617\n",
            "Total Timesteps: 54000 Episode Num: 54 Reward: 4.854877243885007\n",
            "Total Timesteps: 55000 Episode Num: 55 Reward: 177.03352504698404\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 170.651083\n",
            "---------------------------------------\n",
            "Total Timesteps: 56000 Episode Num: 56 Reward: -908.8529693391384\n",
            "Total Timesteps: 57000 Episode Num: 57 Reward: -983.4725384351774\n",
            "Total Timesteps: 58000 Episode Num: 58 Reward: -342.336101025036\n",
            "Total Timesteps: 59000 Episode Num: 59 Reward: 227.0285095761164\n",
            "Total Timesteps: 60000 Episode Num: 60 Reward: 58.613665073596195\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 177.070773\n",
            "---------------------------------------\n",
            "Total Timesteps: 61000 Episode Num: 61 Reward: 113.55419197870198\n",
            "Total Timesteps: 62000 Episode Num: 62 Reward: 379.3661238929709\n",
            "Total Timesteps: 63000 Episode Num: 63 Reward: 774.2635842614076\n",
            "Total Timesteps: 64000 Episode Num: 64 Reward: 656.4872809840115\n",
            "Total Timesteps: 65000 Episode Num: 65 Reward: 787.5870183021276\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 688.121416\n",
            "---------------------------------------\n",
            "Total Timesteps: 66000 Episode Num: 66 Reward: 635.1381874267904\n",
            "Total Timesteps: 67000 Episode Num: 67 Reward: 691.2338555201628\n",
            "Total Timesteps: 68000 Episode Num: 68 Reward: 598.7888384492948\n",
            "Total Timesteps: 69000 Episode Num: 69 Reward: -852.9073021609182\n",
            "Total Timesteps: 70000 Episode Num: 70 Reward: 587.682500711767\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 513.050646\n",
            "---------------------------------------\n",
            "Total Timesteps: 71000 Episode Num: 71 Reward: 869.8919334962484\n",
            "Total Timesteps: 72000 Episode Num: 72 Reward: 709.8155877442014\n",
            "Total Timesteps: 73000 Episode Num: 73 Reward: -251.91788659311672\n",
            "Total Timesteps: 74000 Episode Num: 74 Reward: 575.9060369156587\n",
            "Total Timesteps: 75000 Episode Num: 75 Reward: 753.8823646421409\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -478.108317\n",
            "---------------------------------------\n",
            "Total Timesteps: 76000 Episode Num: 76 Reward: -497.24031123924567\n",
            "Total Timesteps: 77000 Episode Num: 77 Reward: 721.3149339573715\n",
            "Total Timesteps: 78000 Episode Num: 78 Reward: 720.4646079760578\n",
            "Total Timesteps: 79000 Episode Num: 79 Reward: 716.8363461806024\n",
            "Total Timesteps: 80000 Episode Num: 80 Reward: 672.636716552822\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 368.881671\n",
            "---------------------------------------\n",
            "Total Timesteps: 81000 Episode Num: 81 Reward: 779.2561872162693\n",
            "Total Timesteps: 82000 Episode Num: 82 Reward: 749.5876772204455\n",
            "Total Timesteps: 83000 Episode Num: 83 Reward: 701.2527587184959\n",
            "Total Timesteps: 84000 Episode Num: 84 Reward: 773.5766220646003\n",
            "Total Timesteps: 85000 Episode Num: 85 Reward: 636.5916957971469\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 743.944452\n",
            "---------------------------------------\n",
            "Total Timesteps: 86000 Episode Num: 86 Reward: 515.8568870699587\n",
            "Total Timesteps: 87000 Episode Num: 87 Reward: 831.3626576611989\n",
            "Total Timesteps: 88000 Episode Num: 88 Reward: 878.0133166859542\n",
            "Total Timesteps: 89000 Episode Num: 89 Reward: 810.0256046162934\n",
            "Total Timesteps: 90000 Episode Num: 90 Reward: 698.2621195993214\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 271.828719\n",
            "---------------------------------------\n",
            "Total Timesteps: 91000 Episode Num: 91 Reward: 396.6239608922375\n",
            "Total Timesteps: 92000 Episode Num: 92 Reward: 967.3714451625385\n",
            "Total Timesteps: 93000 Episode Num: 93 Reward: 695.5337476768261\n",
            "Total Timesteps: 94000 Episode Num: 94 Reward: 797.4481639654452\n",
            "Total Timesteps: 95000 Episode Num: 95 Reward: 356.0056684146242\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 539.036353\n",
            "---------------------------------------\n",
            "Total Timesteps: 96000 Episode Num: 96 Reward: 570.0758043734658\n",
            "Total Timesteps: 97000 Episode Num: 97 Reward: 288.19004873282313\n",
            "Total Timesteps: 98000 Episode Num: 98 Reward: 292.0472995727629\n",
            "Total Timesteps: 99000 Episode Num: 99 Reward: 730.7031158117404\n",
            "Total Timesteps: 100000 Episode Num: 100 Reward: 617.0936723715392\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 683.898494\n",
            "---------------------------------------\n",
            "Total Timesteps: 101000 Episode Num: 101 Reward: 816.4089718551454\n",
            "Total Timesteps: 102000 Episode Num: 102 Reward: 671.7844567664253\n",
            "Total Timesteps: 103000 Episode Num: 103 Reward: 987.2628854377233\n",
            "Total Timesteps: 104000 Episode Num: 104 Reward: 1076.0514289276905\n",
            "Total Timesteps: 105000 Episode Num: 105 Reward: 969.0496040029486\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 590.672087\n",
            "---------------------------------------\n",
            "Total Timesteps: 106000 Episode Num: 106 Reward: -109.91449850060758\n",
            "Total Timesteps: 107000 Episode Num: 107 Reward: 929.8367318099549\n",
            "Total Timesteps: 108000 Episode Num: 108 Reward: 878.1451314440022\n",
            "Total Timesteps: 109000 Episode Num: 109 Reward: 1011.9990441923002\n",
            "Total Timesteps: 110000 Episode Num: 110 Reward: 963.1442339188028\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 916.442428\n",
            "---------------------------------------\n",
            "Total Timesteps: 111000 Episode Num: 111 Reward: 832.7150419347544\n",
            "Total Timesteps: 112000 Episode Num: 112 Reward: 964.7290430250622\n",
            "Total Timesteps: 113000 Episode Num: 113 Reward: 1037.549358215158\n",
            "Total Timesteps: 114000 Episode Num: 114 Reward: 1034.1185128431505\n",
            "Total Timesteps: 115000 Episode Num: 115 Reward: 1134.5498474663716\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1292.165144\n",
            "---------------------------------------\n",
            "Total Timesteps: 116000 Episode Num: 116 Reward: 1344.1545785367646\n",
            "Total Timesteps: 117000 Episode Num: 117 Reward: 943.1933511806479\n",
            "Total Timesteps: 118000 Episode Num: 118 Reward: 976.9377410437119\n",
            "Total Timesteps: 119000 Episode Num: 119 Reward: 916.0833733133768\n",
            "Total Timesteps: 120000 Episode Num: 120 Reward: 998.7271583861283\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 855.417420\n",
            "---------------------------------------\n",
            "Total Timesteps: 121000 Episode Num: 121 Reward: 986.7741538568453\n",
            "Total Timesteps: 122000 Episode Num: 122 Reward: 940.0991022829631\n",
            "Total Timesteps: 123000 Episode Num: 123 Reward: 1203.6523808785962\n",
            "Total Timesteps: 124000 Episode Num: 124 Reward: 1043.308450331579\n",
            "Total Timesteps: 125000 Episode Num: 125 Reward: 1262.4968240480284\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1162.151380\n",
            "---------------------------------------\n",
            "Total Timesteps: 126000 Episode Num: 126 Reward: 1103.540416233387\n",
            "Total Timesteps: 127000 Episode Num: 127 Reward: 1320.8367513202393\n",
            "Total Timesteps: 128000 Episode Num: 128 Reward: 1268.3369196403144\n",
            "Total Timesteps: 129000 Episode Num: 129 Reward: 1204.8394654105161\n",
            "Total Timesteps: 130000 Episode Num: 130 Reward: 1077.7928711367827\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1248.592866\n",
            "---------------------------------------\n",
            "Total Timesteps: 131000 Episode Num: 131 Reward: 1161.779607338802\n",
            "Total Timesteps: 132000 Episode Num: 132 Reward: 1534.6771948122243\n",
            "Total Timesteps: 133000 Episode Num: 133 Reward: 1309.2355524456002\n",
            "Total Timesteps: 134000 Episode Num: 134 Reward: 1333.624551641616\n",
            "Total Timesteps: 135000 Episode Num: 135 Reward: 1283.4954088341829\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 789.960151\n",
            "---------------------------------------\n",
            "Total Timesteps: 136000 Episode Num: 136 Reward: 907.4452474461664\n",
            "Total Timesteps: 137000 Episode Num: 137 Reward: 855.3087244345743\n",
            "Total Timesteps: 138000 Episode Num: 138 Reward: 1118.8722232036855\n",
            "Total Timesteps: 139000 Episode Num: 139 Reward: 1291.895450244183\n",
            "Total Timesteps: 140000 Episode Num: 140 Reward: 1153.9273216568974\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1261.388273\n",
            "---------------------------------------\n",
            "Total Timesteps: 141000 Episode Num: 141 Reward: 1207.544359958635\n",
            "Total Timesteps: 142000 Episode Num: 142 Reward: 1354.5920527172364\n",
            "Total Timesteps: 143000 Episode Num: 143 Reward: 1325.2058839180054\n",
            "Total Timesteps: 144000 Episode Num: 144 Reward: 1287.6213002888364\n",
            "Total Timesteps: 145000 Episode Num: 145 Reward: 1342.184112349945\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1372.186439\n",
            "---------------------------------------\n",
            "Total Timesteps: 146000 Episode Num: 146 Reward: 1256.6836872806666\n",
            "Total Timesteps: 147000 Episode Num: 147 Reward: 1385.6316535958708\n",
            "Total Timesteps: 148000 Episode Num: 148 Reward: 1336.5422161922802\n",
            "Total Timesteps: 149000 Episode Num: 149 Reward: 1418.6611188542522\n",
            "Total Timesteps: 150000 Episode Num: 150 Reward: 1281.326715022669\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1415.689256\n",
            "---------------------------------------\n",
            "Total Timesteps: 151000 Episode Num: 151 Reward: 1390.6521939289066\n",
            "Total Timesteps: 152000 Episode Num: 152 Reward: 1431.2634710444167\n",
            "Total Timesteps: 153000 Episode Num: 153 Reward: 1346.4446271758104\n",
            "Total Timesteps: 154000 Episode Num: 154 Reward: 1282.1708161893694\n",
            "Total Timesteps: 155000 Episode Num: 155 Reward: 1360.1853832452157\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1355.687768\n",
            "---------------------------------------\n",
            "Total Timesteps: 156000 Episode Num: 156 Reward: 1175.1779989688876\n",
            "Total Timesteps: 157000 Episode Num: 157 Reward: 1539.9601433075393\n",
            "Total Timesteps: 158000 Episode Num: 158 Reward: 1381.1215837683726\n",
            "Total Timesteps: 159000 Episode Num: 159 Reward: 1509.5833257583363\n",
            "Total Timesteps: 160000 Episode Num: 160 Reward: 1409.1805345552436\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1469.913107\n",
            "---------------------------------------\n",
            "Total Timesteps: 161000 Episode Num: 161 Reward: 1422.1749246103896\n",
            "Total Timesteps: 162000 Episode Num: 162 Reward: 1475.4785884889277\n",
            "Total Timesteps: 163000 Episode Num: 163 Reward: 1416.1288201397106\n",
            "Total Timesteps: 164000 Episode Num: 164 Reward: 1410.4346859394361\n",
            "Total Timesteps: 165000 Episode Num: 165 Reward: 1393.2068598322887\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1402.358471\n",
            "---------------------------------------\n",
            "Total Timesteps: 166000 Episode Num: 166 Reward: 1300.6949456151972\n",
            "Total Timesteps: 167000 Episode Num: 167 Reward: 1463.200150001285\n",
            "Total Timesteps: 168000 Episode Num: 168 Reward: 1288.6002350110907\n",
            "Total Timesteps: 169000 Episode Num: 169 Reward: 1377.365624563919\n",
            "Total Timesteps: 170000 Episode Num: 170 Reward: 1539.5652851544958\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1471.228731\n",
            "---------------------------------------\n",
            "Total Timesteps: 171000 Episode Num: 171 Reward: 1522.3738957777148\n",
            "Total Timesteps: 172000 Episode Num: 172 Reward: 1298.67899184079\n",
            "Total Timesteps: 173000 Episode Num: 173 Reward: 1366.8470726290354\n",
            "Total Timesteps: 174000 Episode Num: 174 Reward: 1294.4266667925901\n",
            "Total Timesteps: 175000 Episode Num: 175 Reward: 1404.1814077118104\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1486.647611\n",
            "---------------------------------------\n",
            "Total Timesteps: 176000 Episode Num: 176 Reward: 1490.2591367287391\n",
            "Total Timesteps: 177000 Episode Num: 177 Reward: 1429.6196249961126\n",
            "Total Timesteps: 178000 Episode Num: 178 Reward: 1428.0338408952794\n",
            "Total Timesteps: 179000 Episode Num: 179 Reward: 1208.4209779196347\n",
            "Total Timesteps: 180000 Episode Num: 180 Reward: 1401.7275114734193\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1449.890339\n",
            "---------------------------------------\n",
            "Total Timesteps: 181000 Episode Num: 181 Reward: 1290.7781463654144\n",
            "Total Timesteps: 182000 Episode Num: 182 Reward: 1368.0389615200625\n",
            "Total Timesteps: 183000 Episode Num: 183 Reward: 1380.19908045416\n",
            "Total Timesteps: 184000 Episode Num: 184 Reward: 1414.2378172745425\n",
            "Total Timesteps: 185000 Episode Num: 185 Reward: 1447.5643218115315\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1397.501730\n",
            "---------------------------------------\n",
            "Total Timesteps: 186000 Episode Num: 186 Reward: 1223.0894253901895\n",
            "Total Timesteps: 187000 Episode Num: 187 Reward: 1481.0683422355269\n",
            "Total Timesteps: 188000 Episode Num: 188 Reward: 1613.9748666350376\n",
            "Total Timesteps: 189000 Episode Num: 189 Reward: 1537.1383272596383\n",
            "Total Timesteps: 190000 Episode Num: 190 Reward: 1472.5687432933162\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1509.253572\n",
            "---------------------------------------\n",
            "Total Timesteps: 191000 Episode Num: 191 Reward: 1491.3242832427986\n",
            "Total Timesteps: 192000 Episode Num: 192 Reward: 1466.3366297828463\n",
            "Total Timesteps: 193000 Episode Num: 193 Reward: 1347.8457023577637\n",
            "Total Timesteps: 194000 Episode Num: 194 Reward: 1550.879103473195\n",
            "Total Timesteps: 195000 Episode Num: 195 Reward: 1350.0768862379043\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1494.841306\n",
            "---------------------------------------\n",
            "Total Timesteps: 196000 Episode Num: 196 Reward: 1535.7353633849825\n",
            "Total Timesteps: 197000 Episode Num: 197 Reward: 1581.9267282046321\n",
            "Total Timesteps: 198000 Episode Num: 198 Reward: 1487.6765826027122\n",
            "Total Timesteps: 199000 Episode Num: 199 Reward: 1461.7259239900193\n",
            "Total Timesteps: 200000 Episode Num: 200 Reward: 1500.3846826210413\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1499.465656\n",
            "---------------------------------------\n",
            "Total Timesteps: 201000 Episode Num: 201 Reward: 1399.4782901592794\n",
            "Total Timesteps: 202000 Episode Num: 202 Reward: 1593.5327103480854\n",
            "Total Timesteps: 203000 Episode Num: 203 Reward: 1531.7644803331552\n",
            "Total Timesteps: 204000 Episode Num: 204 Reward: 1522.925464133782\n",
            "Total Timesteps: 205000 Episode Num: 205 Reward: 1469.1648155956843\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1545.843244\n",
            "---------------------------------------\n",
            "Total Timesteps: 206000 Episode Num: 206 Reward: 1541.4663977534708\n",
            "Total Timesteps: 207000 Episode Num: 207 Reward: 1856.8085725242695\n",
            "Total Timesteps: 208000 Episode Num: 208 Reward: 1619.7001152589014\n",
            "Total Timesteps: 209000 Episode Num: 209 Reward: 1637.8702938404729\n",
            "Total Timesteps: 210000 Episode Num: 210 Reward: 1498.797516656446\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1491.717811\n",
            "---------------------------------------\n",
            "Total Timesteps: 211000 Episode Num: 211 Reward: 1513.9706833274433\n",
            "Total Timesteps: 212000 Episode Num: 212 Reward: 1452.2690345027474\n",
            "Total Timesteps: 213000 Episode Num: 213 Reward: 1904.8159305139782\n",
            "Total Timesteps: 214000 Episode Num: 214 Reward: 1799.2778515253567\n",
            "Total Timesteps: 215000 Episode Num: 215 Reward: 1879.0319061703467\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1800.076840\n",
            "---------------------------------------\n",
            "Total Timesteps: 216000 Episode Num: 216 Reward: 1023.2623002111095\n",
            "Total Timesteps: 217000 Episode Num: 217 Reward: 1751.3720185306765\n",
            "Total Timesteps: 218000 Episode Num: 218 Reward: 1717.424040628571\n",
            "Total Timesteps: 219000 Episode Num: 219 Reward: 1716.5871026314203\n",
            "Total Timesteps: 220000 Episode Num: 220 Reward: 1641.9701913934364\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1735.195525\n",
            "---------------------------------------\n",
            "Total Timesteps: 221000 Episode Num: 221 Reward: 1681.3543511350747\n",
            "Total Timesteps: 222000 Episode Num: 222 Reward: 1888.231021947653\n",
            "Total Timesteps: 223000 Episode Num: 223 Reward: 1575.6021479588305\n",
            "Total Timesteps: 224000 Episode Num: 224 Reward: 1542.2336977080251\n",
            "Total Timesteps: 225000 Episode Num: 225 Reward: 1816.9830870761152\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1748.840461\n",
            "---------------------------------------\n",
            "Total Timesteps: 226000 Episode Num: 226 Reward: 1687.6135520967468\n",
            "Total Timesteps: 227000 Episode Num: 227 Reward: 1548.885779206626\n",
            "Total Timesteps: 228000 Episode Num: 228 Reward: 1557.3611932045317\n",
            "Total Timesteps: 229000 Episode Num: 229 Reward: 1448.0422463424954\n",
            "Total Timesteps: 230000 Episode Num: 230 Reward: 1486.4359621453823\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1734.256666\n",
            "---------------------------------------\n",
            "Total Timesteps: 231000 Episode Num: 231 Reward: 1822.9475603665\n",
            "Total Timesteps: 232000 Episode Num: 232 Reward: 1553.8036594659532\n",
            "Total Timesteps: 233000 Episode Num: 233 Reward: 1478.9589426510502\n",
            "Total Timesteps: 234000 Episode Num: 234 Reward: 1706.5637028592055\n",
            "Total Timesteps: 235000 Episode Num: 235 Reward: 1671.929965114018\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1705.940594\n",
            "---------------------------------------\n",
            "Total Timesteps: 236000 Episode Num: 236 Reward: 1564.4674949477196\n",
            "Total Timesteps: 237000 Episode Num: 237 Reward: 1484.6813524227591\n",
            "Total Timesteps: 238000 Episode Num: 238 Reward: 1714.6746124291362\n",
            "Total Timesteps: 239000 Episode Num: 239 Reward: 1880.7609985389402\n",
            "Total Timesteps: 240000 Episode Num: 240 Reward: 1846.6427044608074\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1675.590393\n",
            "---------------------------------------\n",
            "Total Timesteps: 241000 Episode Num: 241 Reward: 1682.9911360726917\n",
            "Total Timesteps: 242000 Episode Num: 242 Reward: 1744.3487642049745\n",
            "Total Timesteps: 243000 Episode Num: 243 Reward: 1646.70402338077\n",
            "Total Timesteps: 244000 Episode Num: 244 Reward: 1944.3607817336283\n",
            "Total Timesteps: 245000 Episode Num: 245 Reward: 1863.8444588414266\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1768.434381\n",
            "---------------------------------------\n",
            "Total Timesteps: 246000 Episode Num: 246 Reward: 1727.8534211639812\n",
            "Total Timesteps: 247000 Episode Num: 247 Reward: 1846.9997645687317\n",
            "Total Timesteps: 248000 Episode Num: 248 Reward: 1809.5121911034744\n",
            "Total Timesteps: 249000 Episode Num: 249 Reward: 1773.4873265547153\n",
            "Total Timesteps: 250000 Episode Num: 250 Reward: 1759.127094531001\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1718.555277\n",
            "---------------------------------------\n",
            "Total Timesteps: 251000 Episode Num: 251 Reward: 1682.6487264295863\n",
            "Total Timesteps: 252000 Episode Num: 252 Reward: 1816.0312570505366\n",
            "Total Timesteps: 253000 Episode Num: 253 Reward: 1720.291826504898\n",
            "Total Timesteps: 254000 Episode Num: 254 Reward: 1483.4845518186764\n",
            "Total Timesteps: 255000 Episode Num: 255 Reward: 1734.3848019870723\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1769.406346\n",
            "---------------------------------------\n",
            "Total Timesteps: 256000 Episode Num: 256 Reward: 1722.252300018979\n",
            "Total Timesteps: 257000 Episode Num: 257 Reward: 1867.2586300623157\n",
            "Total Timesteps: 258000 Episode Num: 258 Reward: 1817.2437223759932\n",
            "Total Timesteps: 259000 Episode Num: 259 Reward: 1680.1466130150664\n",
            "Total Timesteps: 260000 Episode Num: 260 Reward: 1617.6779479046284\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1906.998466\n",
            "---------------------------------------\n",
            "Total Timesteps: 261000 Episode Num: 261 Reward: 1796.0411307033069\n",
            "Total Timesteps: 262000 Episode Num: 262 Reward: 1457.5835317969334\n",
            "Total Timesteps: 263000 Episode Num: 263 Reward: 1840.0512105823398\n",
            "Total Timesteps: 264000 Episode Num: 264 Reward: 1813.632836347635\n",
            "Total Timesteps: 265000 Episode Num: 265 Reward: 1957.3645428237758\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1909.300250\n",
            "---------------------------------------\n",
            "Total Timesteps: 266000 Episode Num: 266 Reward: 1652.4175708959508\n",
            "Total Timesteps: 267000 Episode Num: 267 Reward: 1880.4282924174638\n",
            "Total Timesteps: 268000 Episode Num: 268 Reward: 1745.8002434620796\n",
            "Total Timesteps: 269000 Episode Num: 269 Reward: 1885.3299042924602\n",
            "Total Timesteps: 270000 Episode Num: 270 Reward: 1794.8470109919624\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1934.925148\n",
            "---------------------------------------\n",
            "Total Timesteps: 271000 Episode Num: 271 Reward: 1895.5299569768617\n",
            "Total Timesteps: 272000 Episode Num: 272 Reward: 1826.7875279660134\n",
            "Total Timesteps: 273000 Episode Num: 273 Reward: 1874.6743803376467\n",
            "Total Timesteps: 274000 Episode Num: 274 Reward: 1918.8241564634723\n",
            "Total Timesteps: 275000 Episode Num: 275 Reward: 1644.499175482951\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1846.591477\n",
            "---------------------------------------\n",
            "Total Timesteps: 276000 Episode Num: 276 Reward: 1805.4958728422698\n",
            "Total Timesteps: 277000 Episode Num: 277 Reward: 1632.1206822411773\n",
            "Total Timesteps: 278000 Episode Num: 278 Reward: 1625.891009722873\n",
            "Total Timesteps: 279000 Episode Num: 279 Reward: 1800.8634708333543\n",
            "Total Timesteps: 280000 Episode Num: 280 Reward: 1691.3868213153444\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1930.059694\n",
            "---------------------------------------\n",
            "Total Timesteps: 281000 Episode Num: 281 Reward: 1899.177893093981\n",
            "Total Timesteps: 282000 Episode Num: 282 Reward: 1731.9897421094497\n",
            "Total Timesteps: 283000 Episode Num: 283 Reward: 2001.4125983903243\n",
            "Total Timesteps: 284000 Episode Num: 284 Reward: 1831.8144977652837\n",
            "Total Timesteps: 285000 Episode Num: 285 Reward: 1806.2482421968268\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1873.214608\n",
            "---------------------------------------\n",
            "Total Timesteps: 286000 Episode Num: 286 Reward: 1884.3450285656816\n",
            "Total Timesteps: 287000 Episode Num: 287 Reward: 1957.3838219282109\n",
            "Total Timesteps: 288000 Episode Num: 288 Reward: 1907.157908999408\n",
            "Total Timesteps: 289000 Episode Num: 289 Reward: 1917.0600562307682\n",
            "Total Timesteps: 290000 Episode Num: 290 Reward: 1726.611655888137\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1896.144378\n",
            "---------------------------------------\n",
            "Total Timesteps: 291000 Episode Num: 291 Reward: 1896.8607579551983\n",
            "Total Timesteps: 292000 Episode Num: 292 Reward: 1919.248374610312\n",
            "Total Timesteps: 293000 Episode Num: 293 Reward: 1907.2996374131492\n",
            "Total Timesteps: 294000 Episode Num: 294 Reward: 1946.1853453877936\n",
            "Total Timesteps: 295000 Episode Num: 295 Reward: 1716.4336151354466\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1986.782125\n",
            "---------------------------------------\n",
            "Total Timesteps: 296000 Episode Num: 296 Reward: 1980.0221148630799\n",
            "Total Timesteps: 297000 Episode Num: 297 Reward: 2009.1897921744967\n",
            "Total Timesteps: 298000 Episode Num: 298 Reward: 2039.3252560158721\n",
            "Total Timesteps: 299000 Episode Num: 299 Reward: 1971.2796586837394\n",
            "Total Timesteps: 300000 Episode Num: 300 Reward: 2009.4846607532313\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2046.058988\n",
            "---------------------------------------\n",
            "Total Timesteps: 301000 Episode Num: 301 Reward: 1976.551975179371\n",
            "Total Timesteps: 302000 Episode Num: 302 Reward: 1918.8997665993243\n",
            "Total Timesteps: 303000 Episode Num: 303 Reward: 2032.1084334749296\n",
            "Total Timesteps: 304000 Episode Num: 304 Reward: 2055.684157539225\n",
            "Total Timesteps: 305000 Episode Num: 305 Reward: 1862.0198681701318\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2081.193578\n",
            "---------------------------------------\n",
            "Total Timesteps: 306000 Episode Num: 306 Reward: 2026.913176509866\n",
            "Total Timesteps: 307000 Episode Num: 307 Reward: 1992.4363892647061\n",
            "Total Timesteps: 308000 Episode Num: 308 Reward: 2073.16909235878\n",
            "Total Timesteps: 309000 Episode Num: 309 Reward: 2019.9776117299396\n",
            "Total Timesteps: 310000 Episode Num: 310 Reward: 1914.766237075344\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2015.658411\n",
            "---------------------------------------\n",
            "Total Timesteps: 311000 Episode Num: 311 Reward: 2021.8081487702318\n",
            "Total Timesteps: 312000 Episode Num: 312 Reward: 2013.5872000018048\n",
            "Total Timesteps: 313000 Episode Num: 313 Reward: 2103.139757524029\n",
            "Total Timesteps: 314000 Episode Num: 314 Reward: 1957.2790068503584\n",
            "Total Timesteps: 315000 Episode Num: 315 Reward: 2077.4485199870633\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2053.570208\n",
            "---------------------------------------\n",
            "Total Timesteps: 316000 Episode Num: 316 Reward: 2068.616913137302\n",
            "Total Timesteps: 317000 Episode Num: 317 Reward: 2023.4521416319137\n",
            "Total Timesteps: 318000 Episode Num: 318 Reward: 1931.9004714213427\n",
            "Total Timesteps: 319000 Episode Num: 319 Reward: 1989.5722648573178\n",
            "Total Timesteps: 320000 Episode Num: 320 Reward: 2074.9129630456337\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1946.326854\n",
            "---------------------------------------\n",
            "Total Timesteps: 321000 Episode Num: 321 Reward: 1904.2731055191575\n",
            "Total Timesteps: 322000 Episode Num: 322 Reward: 1969.7121446088343\n",
            "Total Timesteps: 323000 Episode Num: 323 Reward: 2065.212309056222\n",
            "Total Timesteps: 324000 Episode Num: 324 Reward: 2014.170654856938\n",
            "Total Timesteps: 325000 Episode Num: 325 Reward: 1991.674457678617\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2112.515866\n",
            "---------------------------------------\n",
            "Total Timesteps: 326000 Episode Num: 326 Reward: 1888.2631784837727\n",
            "Total Timesteps: 327000 Episode Num: 327 Reward: 2053.952468892762\n",
            "Total Timesteps: 328000 Episode Num: 328 Reward: 2048.5158181839297\n",
            "Total Timesteps: 329000 Episode Num: 329 Reward: 2097.4557214417086\n",
            "Total Timesteps: 330000 Episode Num: 330 Reward: 2114.0953293530665\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2168.890038\n",
            "---------------------------------------\n",
            "Total Timesteps: 331000 Episode Num: 331 Reward: 2112.250099576176\n",
            "Total Timesteps: 332000 Episode Num: 332 Reward: 2066.9830582561617\n",
            "Total Timesteps: 333000 Episode Num: 333 Reward: 2047.439865247229\n",
            "Total Timesteps: 334000 Episode Num: 334 Reward: 2077.427335304417\n",
            "Total Timesteps: 335000 Episode Num: 335 Reward: 2074.314289263363\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2073.369261\n",
            "---------------------------------------\n",
            "Total Timesteps: 336000 Episode Num: 336 Reward: 1963.307418913723\n",
            "Total Timesteps: 337000 Episode Num: 337 Reward: 2150.9416840509066\n",
            "Total Timesteps: 338000 Episode Num: 338 Reward: 2040.940377085539\n",
            "Total Timesteps: 339000 Episode Num: 339 Reward: 2087.080065720385\n",
            "Total Timesteps: 340000 Episode Num: 340 Reward: 2096.367472475754\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2183.322947\n",
            "---------------------------------------\n",
            "Total Timesteps: 341000 Episode Num: 341 Reward: 2125.417586987748\n",
            "Total Timesteps: 342000 Episode Num: 342 Reward: 2054.4362225535965\n",
            "Total Timesteps: 343000 Episode Num: 343 Reward: 1980.0719147502086\n",
            "Total Timesteps: 344000 Episode Num: 344 Reward: 2129.958953712122\n",
            "Total Timesteps: 345000 Episode Num: 345 Reward: 2168.6108459542893\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2177.146927\n",
            "---------------------------------------\n",
            "Total Timesteps: 346000 Episode Num: 346 Reward: 2105.622678409025\n",
            "Total Timesteps: 347000 Episode Num: 347 Reward: 2029.9646005080203\n",
            "Total Timesteps: 348000 Episode Num: 348 Reward: 2094.4052133027335\n",
            "Total Timesteps: 349000 Episode Num: 349 Reward: 2080.193791929397\n",
            "Total Timesteps: 350000 Episode Num: 350 Reward: 2047.8437669591333\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2065.242544\n",
            "---------------------------------------\n",
            "Total Timesteps: 351000 Episode Num: 351 Reward: 2029.6109611407503\n",
            "Total Timesteps: 352000 Episode Num: 352 Reward: 2172.521885369981\n",
            "Total Timesteps: 353000 Episode Num: 353 Reward: 2039.6642829941836\n",
            "Total Timesteps: 354000 Episode Num: 354 Reward: 2113.4745507560647\n",
            "Total Timesteps: 355000 Episode Num: 355 Reward: 2083.268124530024\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2147.001183\n",
            "---------------------------------------\n",
            "Total Timesteps: 356000 Episode Num: 356 Reward: 2093.8178772364095\n",
            "Total Timesteps: 357000 Episode Num: 357 Reward: 2099.4779424933927\n",
            "Total Timesteps: 358000 Episode Num: 358 Reward: 2053.982922087332\n",
            "Total Timesteps: 359000 Episode Num: 359 Reward: 2154.0391762050663\n",
            "Total Timesteps: 360000 Episode Num: 360 Reward: 2144.3187467615044\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2114.223487\n",
            "---------------------------------------\n",
            "Total Timesteps: 361000 Episode Num: 361 Reward: 2085.7543773771617\n",
            "Total Timesteps: 362000 Episode Num: 362 Reward: 2132.93184202566\n",
            "Total Timesteps: 363000 Episode Num: 363 Reward: 2173.2839999556204\n",
            "Total Timesteps: 364000 Episode Num: 364 Reward: 2170.326082641317\n",
            "Total Timesteps: 365000 Episode Num: 365 Reward: 2025.4126236273785\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2105.519088\n",
            "---------------------------------------\n",
            "Total Timesteps: 366000 Episode Num: 366 Reward: 2055.783548271116\n",
            "Total Timesteps: 367000 Episode Num: 367 Reward: 2129.667177750185\n",
            "Total Timesteps: 368000 Episode Num: 368 Reward: 2123.3603199603026\n",
            "Total Timesteps: 369000 Episode Num: 369 Reward: 2146.1952667888095\n",
            "Total Timesteps: 370000 Episode Num: 370 Reward: 2084.9867780087097\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2175.834190\n",
            "---------------------------------------\n",
            "Total Timesteps: 371000 Episode Num: 371 Reward: 2102.4127445340755\n",
            "Total Timesteps: 372000 Episode Num: 372 Reward: 2165.705155214293\n",
            "Total Timesteps: 373000 Episode Num: 373 Reward: 2061.3926889364966\n",
            "Total Timesteps: 374000 Episode Num: 374 Reward: 2164.9145252153753\n",
            "Total Timesteps: 375000 Episode Num: 375 Reward: 2097.865459721348\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2213.096042\n",
            "---------------------------------------\n",
            "Total Timesteps: 376000 Episode Num: 376 Reward: 2129.369884347068\n",
            "Total Timesteps: 377000 Episode Num: 377 Reward: 2135.6217810137164\n",
            "Total Timesteps: 378000 Episode Num: 378 Reward: 2132.3469645208156\n",
            "Total Timesteps: 379000 Episode Num: 379 Reward: 2101.093276868234\n",
            "Total Timesteps: 380000 Episode Num: 380 Reward: 2096.278931383927\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2264.022626\n",
            "---------------------------------------\n",
            "Total Timesteps: 381000 Episode Num: 381 Reward: 2186.311927083318\n",
            "Total Timesteps: 382000 Episode Num: 382 Reward: 2102.3566456721665\n",
            "Total Timesteps: 383000 Episode Num: 383 Reward: 2040.7488866809276\n",
            "Total Timesteps: 384000 Episode Num: 384 Reward: 2202.5848001460513\n",
            "Total Timesteps: 385000 Episode Num: 385 Reward: 2117.9188506347145\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2274.578877\n",
            "---------------------------------------\n",
            "Total Timesteps: 386000 Episode Num: 386 Reward: 2187.05528520093\n",
            "Total Timesteps: 387000 Episode Num: 387 Reward: 2074.515307533056\n",
            "Total Timesteps: 388000 Episode Num: 388 Reward: 2159.398149644844\n",
            "Total Timesteps: 389000 Episode Num: 389 Reward: 2153.5804625182327\n",
            "Total Timesteps: 390000 Episode Num: 390 Reward: 2125.7293205716837\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2237.754361\n",
            "---------------------------------------\n",
            "Total Timesteps: 391000 Episode Num: 391 Reward: 2208.839618942069\n",
            "Total Timesteps: 392000 Episode Num: 392 Reward: 2274.6649061807757\n",
            "Total Timesteps: 393000 Episode Num: 393 Reward: 2222.83682277636\n",
            "Total Timesteps: 394000 Episode Num: 394 Reward: 2070.189815056962\n",
            "Total Timesteps: 395000 Episode Num: 395 Reward: 2129.420101109047\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2247.486628\n",
            "---------------------------------------\n",
            "Total Timesteps: 396000 Episode Num: 396 Reward: 2210.179852333135\n",
            "Total Timesteps: 397000 Episode Num: 397 Reward: 2112.440975674594\n",
            "Total Timesteps: 398000 Episode Num: 398 Reward: 1937.465702379194\n",
            "Total Timesteps: 399000 Episode Num: 399 Reward: 2096.09003913611\n",
            "Total Timesteps: 400000 Episode Num: 400 Reward: 2190.039506418859\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2130.486249\n",
            "---------------------------------------\n",
            "Total Timesteps: 401000 Episode Num: 401 Reward: 2123.8003324793217\n",
            "Total Timesteps: 402000 Episode Num: 402 Reward: 2088.4330654054997\n",
            "Total Timesteps: 403000 Episode Num: 403 Reward: 2104.8728407795606\n",
            "Total Timesteps: 404000 Episode Num: 404 Reward: 2048.883699379673\n",
            "Total Timesteps: 405000 Episode Num: 405 Reward: 1960.1090590956767\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2188.429389\n",
            "---------------------------------------\n",
            "Total Timesteps: 406000 Episode Num: 406 Reward: 2195.2792575967214\n",
            "Total Timesteps: 407000 Episode Num: 407 Reward: 2223.36839027374\n",
            "Total Timesteps: 408000 Episode Num: 408 Reward: 2132.9084378821312\n",
            "Total Timesteps: 409000 Episode Num: 409 Reward: 2052.7642136575278\n",
            "Total Timesteps: 410000 Episode Num: 410 Reward: 2033.5772071911515\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2021.789101\n",
            "---------------------------------------\n",
            "Total Timesteps: 411000 Episode Num: 411 Reward: 1996.164350166792\n",
            "Total Timesteps: 412000 Episode Num: 412 Reward: 2070.3697034691095\n",
            "Total Timesteps: 413000 Episode Num: 413 Reward: 2147.9646834909963\n",
            "Total Timesteps: 414000 Episode Num: 414 Reward: 2185.5279847403117\n",
            "Total Timesteps: 415000 Episode Num: 415 Reward: 2161.8472116476064\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2141.737811\n",
            "---------------------------------------\n",
            "Total Timesteps: 416000 Episode Num: 416 Reward: 2079.0316946489443\n",
            "Total Timesteps: 417000 Episode Num: 417 Reward: 2125.181298423118\n",
            "Total Timesteps: 418000 Episode Num: 418 Reward: 2141.2380788399555\n",
            "Total Timesteps: 419000 Episode Num: 419 Reward: 2086.81193023798\n",
            "Total Timesteps: 420000 Episode Num: 420 Reward: 2192.2776703949303\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2255.413980\n",
            "---------------------------------------\n",
            "Total Timesteps: 421000 Episode Num: 421 Reward: 2194.386226496354\n",
            "Total Timesteps: 422000 Episode Num: 422 Reward: 2151.7614165060045\n",
            "Total Timesteps: 423000 Episode Num: 423 Reward: 2107.2576525103714\n",
            "Total Timesteps: 424000 Episode Num: 424 Reward: 2127.9115183008703\n",
            "Total Timesteps: 425000 Episode Num: 425 Reward: 2180.386155286953\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2180.435433\n",
            "---------------------------------------\n",
            "Total Timesteps: 426000 Episode Num: 426 Reward: 2132.1696255545367\n",
            "Total Timesteps: 427000 Episode Num: 427 Reward: 2193.085532592452\n",
            "Total Timesteps: 428000 Episode Num: 428 Reward: 2210.1734450811373\n",
            "Total Timesteps: 429000 Episode Num: 429 Reward: 2142.1135413368197\n",
            "Total Timesteps: 430000 Episode Num: 430 Reward: 2125.4320359544827\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2213.151395\n",
            "---------------------------------------\n",
            "Total Timesteps: 431000 Episode Num: 431 Reward: 2185.7851364861963\n",
            "Total Timesteps: 432000 Episode Num: 432 Reward: 2213.7398719557427\n",
            "Total Timesteps: 433000 Episode Num: 433 Reward: 2212.391482790521\n",
            "Total Timesteps: 434000 Episode Num: 434 Reward: 2185.862404231124\n",
            "Total Timesteps: 435000 Episode Num: 435 Reward: 2161.313509562813\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2166.183208\n",
            "---------------------------------------\n",
            "Total Timesteps: 436000 Episode Num: 436 Reward: 2215.86997147536\n",
            "Total Timesteps: 437000 Episode Num: 437 Reward: 2118.639502071482\n",
            "Total Timesteps: 438000 Episode Num: 438 Reward: 2178.407980339808\n",
            "Total Timesteps: 439000 Episode Num: 439 Reward: 1978.3592284075573\n",
            "Total Timesteps: 440000 Episode Num: 440 Reward: 2200.555716296779\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2100.069966\n",
            "---------------------------------------\n",
            "Total Timesteps: 441000 Episode Num: 441 Reward: 2145.013728182561\n",
            "Total Timesteps: 442000 Episode Num: 442 Reward: 2172.994669402836\n",
            "Total Timesteps: 443000 Episode Num: 443 Reward: 2165.50387351499\n",
            "Total Timesteps: 444000 Episode Num: 444 Reward: 2178.439720850573\n",
            "Total Timesteps: 445000 Episode Num: 445 Reward: 2139.790554054067\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2179.762508\n",
            "---------------------------------------\n",
            "Total Timesteps: 446000 Episode Num: 446 Reward: 2114.160196606688\n",
            "Total Timesteps: 447000 Episode Num: 447 Reward: 2028.9748941946805\n",
            "Total Timesteps: 448000 Episode Num: 448 Reward: 2132.8226571846103\n",
            "Total Timesteps: 449000 Episode Num: 449 Reward: 2164.2792676649397\n",
            "Total Timesteps: 450000 Episode Num: 450 Reward: 2143.0515488689616\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2203.606163\n",
            "---------------------------------------\n",
            "Total Timesteps: 451000 Episode Num: 451 Reward: 2152.987000727424\n",
            "Total Timesteps: 452000 Episode Num: 452 Reward: 2203.3676209118807\n",
            "Total Timesteps: 453000 Episode Num: 453 Reward: 2149.0307024171093\n",
            "Total Timesteps: 454000 Episode Num: 454 Reward: 2080.63344065946\n",
            "Total Timesteps: 455000 Episode Num: 455 Reward: 2234.4305062299245\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2156.534858\n",
            "---------------------------------------\n",
            "Total Timesteps: 456000 Episode Num: 456 Reward: 2079.480954413351\n",
            "Total Timesteps: 457000 Episode Num: 457 Reward: 2092.05144692571\n",
            "Total Timesteps: 458000 Episode Num: 458 Reward: 2221.6768309206072\n",
            "Total Timesteps: 459000 Episode Num: 459 Reward: 2179.4962505575086\n",
            "Total Timesteps: 460000 Episode Num: 460 Reward: 2166.693000361608\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2216.501081\n",
            "---------------------------------------\n",
            "Total Timesteps: 461000 Episode Num: 461 Reward: 2168.3287963487287\n",
            "Total Timesteps: 462000 Episode Num: 462 Reward: 2026.4751344581384\n",
            "Total Timesteps: 463000 Episode Num: 463 Reward: 2210.825799542631\n",
            "Total Timesteps: 464000 Episode Num: 464 Reward: 2221.79895788039\n",
            "Total Timesteps: 465000 Episode Num: 465 Reward: 2011.8560373018554\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2146.222516\n",
            "---------------------------------------\n",
            "Total Timesteps: 466000 Episode Num: 466 Reward: 2197.249385071191\n",
            "Total Timesteps: 467000 Episode Num: 467 Reward: 2116.165425602719\n",
            "Total Timesteps: 468000 Episode Num: 468 Reward: 2166.034292081932\n",
            "Total Timesteps: 469000 Episode Num: 469 Reward: 2089.9468570506306\n",
            "Total Timesteps: 470000 Episode Num: 470 Reward: 2069.2092951557497\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2179.278866\n",
            "---------------------------------------\n",
            "Total Timesteps: 471000 Episode Num: 471 Reward: 2193.308207955975\n",
            "Total Timesteps: 472000 Episode Num: 472 Reward: 2015.389230519624\n",
            "Total Timesteps: 473000 Episode Num: 473 Reward: 2211.528001245823\n",
            "Total Timesteps: 474000 Episode Num: 474 Reward: 2149.794695672209\n",
            "Total Timesteps: 475000 Episode Num: 475 Reward: 2225.6186407453147\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2218.950124\n",
            "---------------------------------------\n",
            "Total Timesteps: 476000 Episode Num: 476 Reward: 2119.008365541341\n",
            "Total Timesteps: 477000 Episode Num: 477 Reward: 2234.8879759359575\n",
            "Total Timesteps: 478000 Episode Num: 478 Reward: 2146.3811378787955\n",
            "Total Timesteps: 479000 Episode Num: 479 Reward: 2191.7765615038275\n",
            "Total Timesteps: 480000 Episode Num: 480 Reward: 2242.4607734913193\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2258.159128\n",
            "---------------------------------------\n",
            "Total Timesteps: 481000 Episode Num: 481 Reward: 2195.039892250138\n",
            "Total Timesteps: 482000 Episode Num: 482 Reward: 2224.073487648387\n",
            "Total Timesteps: 483000 Episode Num: 483 Reward: 2236.0070062496047\n",
            "Total Timesteps: 484000 Episode Num: 484 Reward: 2216.891447741414\n",
            "Total Timesteps: 485000 Episode Num: 485 Reward: 2199.7282190965016\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2283.878936\n",
            "---------------------------------------\n",
            "Total Timesteps: 486000 Episode Num: 486 Reward: 2198.352989587912\n",
            "Total Timesteps: 487000 Episode Num: 487 Reward: 2309.3951970723783\n",
            "Total Timesteps: 488000 Episode Num: 488 Reward: 2195.5519843943302\n",
            "Total Timesteps: 489000 Episode Num: 489 Reward: 2181.4482348139495\n",
            "Total Timesteps: 490000 Episode Num: 490 Reward: 1953.1145584453436\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2212.998135\n",
            "---------------------------------------\n",
            "Total Timesteps: 491000 Episode Num: 491 Reward: 2176.8465016397276\n",
            "Total Timesteps: 492000 Episode Num: 492 Reward: 2155.076765848707\n",
            "Total Timesteps: 493000 Episode Num: 493 Reward: 2144.0255584009465\n",
            "Total Timesteps: 494000 Episode Num: 494 Reward: 2168.674416314285\n",
            "Total Timesteps: 495000 Episode Num: 495 Reward: 2217.554651782871\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2120.024217\n",
            "---------------------------------------\n",
            "Total Timesteps: 496000 Episode Num: 496 Reward: 2073.051263429009\n",
            "Total Timesteps: 497000 Episode Num: 497 Reward: 2257.6326296060292\n",
            "Total Timesteps: 498000 Episode Num: 498 Reward: 2112.7171475260257\n",
            "Total Timesteps: 499000 Episode Num: 499 Reward: 2185.16300054665\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2305.003548\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi6e2-_pu05e",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW4d1YAMqif1",
        "colab_type": "code",
        "outputId": "467fd0cd-3ee4-4385-eb52-9141033d672c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
        "    return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1\n",
        "\n",
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward\n",
        "\n",
        "env_name = \"HalfCheetahBulletEnv-v0\"\n",
        "seed = 0\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "eval_episodes = 10\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "max_episode_steps = env._max_episode_steps\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_HalfCheetahBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2324.014128\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcnexWrW4a8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}